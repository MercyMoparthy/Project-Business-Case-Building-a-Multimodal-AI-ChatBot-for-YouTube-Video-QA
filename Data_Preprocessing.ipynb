{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca79a96b",
   "metadata": {},
   "source": [
    "## Data PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ab8c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.3.26-py3-none-any.whl (1.0 MB)\n",
      "     ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "     ------------ --------------------------- 0.3/1.0 MB 6.3 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 0.6/1.0 MB 5.9 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 0.8/1.0 MB 5.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.0/1.0 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.0/1.0 MB 5.3 MB/s eta 0:00:00\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.7-cp310-cp310-win_amd64.whl (14.9 MB)\n",
      "     ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/14.9 MB 14.7 MB/s eta 0:00:01\n",
      "     -- ------------------------------------- 1.1/14.9 MB 13.9 MB/s eta 0:00:01\n",
      "     ---- ----------------------------------- 1.7/14.9 MB 13.4 MB/s eta 0:00:01\n",
      "     ------ --------------------------------- 2.3/14.9 MB 13.5 MB/s eta 0:00:01\n",
      "     ------- -------------------------------- 2.9/14.9 MB 13.4 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 3.6/14.9 MB 13.4 MB/s eta 0:00:01\n",
      "     ----------- ---------------------------- 4.2/14.9 MB 13.3 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 4.8/14.9 MB 13.2 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 5.4/14.9 MB 13.2 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 6.0/14.9 MB 13.2 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 6.6/14.9 MB 13.2 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 7.2/14.9 MB 13.2 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 7.8/14.9 MB 13.2 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 8.4/14.9 MB 13.1 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 9.0/14.9 MB 13.1 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 9.7/14.9 MB 13.2 MB/s eta 0:00:01\n",
      "     -------------------------- ------------ 10.3/14.9 MB 13.1 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 10.9/14.9 MB 13.1 MB/s eta 0:00:01\n",
      "     ----------------------------- --------- 11.4/14.9 MB 13.1 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 12.1/14.9 MB 13.1 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 12.8/14.9 MB 13.1 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 13.4/14.9 MB 13.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 14.0/14.9 MB 13.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  14.6/14.9 MB 13.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  14.9/14.9 MB 13.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  14.9/14.9 MB 13.1 MB/s eta 0:00:01\n",
      "     --------------------------------------- 14.9/14.9 MB 11.7 MB/s eta 0:00:00\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     -------------- ------------------------- 0.5/1.5 MB 16.6 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 1.2/1.5 MB 15.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 12.0 MB/s eta 0:00:00\n",
      "Collecting pinecone-client\n",
      "  Downloading pinecone_client-6.0.0-py3-none-any.whl (6.7 kB)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Collecting PyYAML>=5.3\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-win_amd64.whl (161 kB)\n",
      "     ---------------------------------------- 0.0/161.8 kB ? eta -:--:--\n",
      "     -------------------------------------- 161.8/161.8 kB 9.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from langchain) (2.32.4)\n",
      "Collecting langchain-core<1.0.0,>=0.3.66\n",
      "  Downloading langchain_core-0.3.66-py3-none-any.whl (438 kB)\n",
      "     ---------------------------------------- 0.0/438.9 kB ? eta -:--:--\n",
      "     ------------------------------ ------ 358.4/438.9 kB 10.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- 438.9/438.9 kB 9.1 MB/s eta 0:00:00\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4\n",
      "  Downloading sqlalchemy-2.0.41-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "     ------ --------------------------------- 0.3/2.1 MB 7.3 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 0.6/2.1 MB 6.7 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 0.9/2.1 MB 6.5 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 1.2/2.1 MB 7.0 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 1.5/2.1 MB 6.7 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 1.8/2.1 MB 6.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.1/2.1 MB 6.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.1/2.1 MB 6.1 MB/s eta 0:00:00\n",
      "Collecting async-timeout<5.0.0,>=4.0.0\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "     ---------------------------------------- 0.0/444.8 kB ? eta -:--:--\n",
      "     -------------------------------------  440.3/444.8 kB 9.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 444.8/444.8 kB 9.2 MB/s eta 0:00:00\n",
      "Collecting langsmith>=0.1.17\n",
      "  Downloading langsmith-0.4.2-py3-none-any.whl (367 kB)\n",
      "     ---------------------------------------- 0.0/367.7 kB ? eta -:--:--\n",
      "     -------------------------------- ----- 317.4/367.7 kB 6.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- 367.7/367.7 kB 5.8 MB/s eta 0:00:00\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.11-cp310-cp310-win_amd64.whl (39 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0\n",
      "  Downloading typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "     ---------------------------------------- 0.0/46.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 46.3/46.3 kB ? eta 0:00:00\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from spacy) (25.0)\n",
      "Collecting weasel<0.5.0,>=0.1.0\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "     ---------------------------------------- 0.0/50.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 50.3/50.3 kB 2.7 MB/s eta 0:00:00\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.10-cp310-cp310-win_amd64.whl (117 kB)\n",
      "     ---------------------------------------- 0.0/117.4 kB ? eta -:--:--\n",
      "     -------------------------------------- 117.4/117.4 kB 6.7 MB/s eta 0:00:00\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from spacy) (2.2.6)\n",
      "Collecting thinc<8.4.0,>=8.3.4\n",
      "  Downloading thinc-8.3.6-cp310-cp310-win_amd64.whl (1.8 MB)\n",
      "     ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "     ------- -------------------------------- 0.3/1.8 MB 7.2 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 0.6/1.8 MB 6.8 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 0.9/1.8 MB 6.6 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 1.3/1.8 MB 6.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 1.6/1.8 MB 6.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.8/1.8 MB 6.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.8/1.8 MB 5.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from spacy) (65.5.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "     ---------------------------------------- 0.0/183.0 kB ? eta -:--:--\n",
      "     -------------------------------------- 183.0/183.0 kB 5.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.13-cp310-cp310-win_amd64.whl (24 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.5.1-cp310-cp310-win_amd64.whl (632 kB)\n",
      "     ---------------------------------------- 0.0/632.3 kB ? eta -:--:--\n",
      "     ---------- --------------------------- 174.1/632.3 kB 5.3 MB/s eta 0:00:01\n",
      "     ------------------- ------------------ 317.4/632.3 kB 3.9 MB/s eta 0:00:01\n",
      "     --------------------------- ---------- 450.6/632.3 kB 3.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- -- 593.9/632.3 kB 3.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- 632.3/632.3 kB 3.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "     ---------------------------------------- 0.0/307.7 kB ? eta -:--:--\n",
      "     -------------------------------------- 307.7/307.7 kB 9.3 MB/s eta 0:00:00\n",
      "Collecting click\n",
      "  Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "     ---------------------------------------- 0.0/102.2 kB ? eta -:--:--\n",
      "     -------------------------------------- 102.2/102.2 kB 5.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from pinecone-client) (4.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from pinecone-client) (2.9.0.post0)\n",
      "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7\n",
      "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from pinecone-client) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from pinecone-client) (2025.6.15)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting packaging>=20.0\n",
      "  Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 0.0/65.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 65.5/65.5 kB ? eta 0:00:00\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Collecting language-data>=1.2\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "     ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.2/5.4 MB 4.6 MB/s eta 0:00:02\n",
      "     -- ------------------------------------- 0.4/5.4 MB 4.0 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 0.5/5.4 MB 3.8 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 0.7/5.4 MB 3.7 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 1.0/5.4 MB 4.2 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 1.7/5.4 MB 5.7 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 2.3/5.4 MB 6.7 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 2.9/5.4 MB 7.5 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 3.5/5.4 MB 8.1 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 4.1/5.4 MB 8.5 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 4.7/5.4 MB 8.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.3/5.4 MB 9.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 5.4/5.4 MB 9.1 MB/s eta 0:00:00\n",
      "Collecting httpx<1,>=0.23.0\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "     ---------------------------------------- 0.0/54.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 54.5/54.5 kB ? eta 0:00:00\n",
      "Collecting orjson<4.0.0,>=3.9.14\n",
      "  Downloading orjson-3.10.18-cp310-cp310-win_amd64.whl (134 kB)\n",
      "     ---------------------------------------- 0.0/134.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 134.6/134.6 kB 7.8 MB/s eta 0:00:00\n",
      "Collecting zstandard<0.24.0,>=0.23.0\n",
      "  Downloading zstandard-0.23.0-cp310-cp310-win_amd64.whl (495 kB)\n",
      "     ---------------------------------------- 0.0/495.5 kB ? eta -:--:--\n",
      "     ------------------------------------  491.5/495.5 kB 15.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- 495.5/495.5 kB 10.3 MB/s eta 0:00:00\n",
      "Collecting pydantic-core==2.33.2\n",
      "  Downloading pydantic_core-2.33.2-cp310-cp310-win_amd64.whl (2.0 MB)\n",
      "     ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "     -------- ------------------------------- 0.4/2.0 MB 8.7 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 0.7/2.0 MB 7.4 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 1.0/2.0 MB 6.8 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 1.2/2.0 MB 6.6 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 1.5/2.0 MB 6.4 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 1.8/2.0 MB 6.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.0/2.0 MB 5.9 MB/s eta 0:00:00\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting typing-inspection>=0.4.0\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from python-dateutil>=2.5.3->pinecone-client) (1.17.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Collecting greenlet>=1\n",
      "  Downloading greenlet-3.2.3-cp310-cp310-win_amd64.whl (296 kB)\n",
      "     ---------------------------------------- 0.0/296.6 kB ? eta -:--:--\n",
      "     --------------------------------- --- 266.2/296.6 kB 16.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- 296.6/296.6 kB 4.5 MB/s eta 0:00:00\n",
      "Collecting blis<1.4.0,>=1.3.0\n",
      "  Downloading blis-1.3.0-cp310-cp310-win_amd64.whl (6.2 MB)\n",
      "     ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "     ---- ----------------------------------- 0.7/6.2 MB 14.2 MB/s eta 0:00:01\n",
      "     ------- -------------------------------- 1.2/6.2 MB 12.3 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 1.7/6.2 MB 12.0 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 2.2/6.2 MB 11.4 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 2.7/6.2 MB 11.3 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 4.0/6.2 MB 10.2 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 4.6/6.2 MB 10.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 5.0/6.2 MB 10.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 5.6/6.2 MB 10.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  6.1/6.2 MB 10.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 6.2/6.2 MB 10.2 MB/s eta 0:00:00\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Collecting rich>=10.11.0\n",
      "  Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "     ---------------------------------------- 0.0/243.2 kB ? eta -:--:--\n",
      "     -------------------------------------- 243.2/243.2 kB 7.3 MB/s eta 0:00:00\n",
      "Collecting shellingham>=1.3.0\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0\n",
      "  Downloading cloudpathlib-0.21.1-py3-none-any.whl (52 kB)\n",
      "     ---------------------------------------- 0.0/52.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 52.8/52.8 kB ? eta 0:00:00\n",
      "Collecting smart-open<8.0.0,>=5.2.1\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "     ---------------------------------------- 0.0/61.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 61.7/61.7 kB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Collecting anyio\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Collecting httpcore==1.*\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Collecting h11>=0.16\n",
      "  Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Collecting jsonpointer>=1.9\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting marisa-trie>=1.1.0\n",
      "  Downloading marisa_trie-1.2.1-cp310-cp310-win_amd64.whl (151 kB)\n",
      "     ---------------------------------------- 0.0/151.9 kB ? eta -:--:--\n",
      "     -------------------------------------- 151.9/151.9 kB 9.4 MB/s eta 0:00:00\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "     ---------------------------------------- 0.0/87.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 87.5/87.5 kB 4.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Collecting wrapt\n",
      "  Downloading wrapt-1.17.2-cp310-cp310-win_amd64.whl (38 kB)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting sniffio>=1.1\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.0)\n",
      "Installing collected packages: cymem, zstandard, wrapt, wasabi, typing-inspection, tenacity, spacy-loggers, spacy-legacy, sniffio, shellingham, PyYAML, python-dotenv, pydantic-core, pinecone-plugin-interface, packaging, orjson, murmurhash, mdurl, marisa-trie, jsonpointer, joblib, h11, greenlet, cloudpathlib, click, catalogue, blis, async-timeout, annotated-types, srsly, SQLAlchemy, smart-open, requests-toolbelt, pydantic, preshed, pinecone-client, nltk, markdown-it-py, language-data, jsonpatch, httpcore, anyio, rich, langcodes, httpx, confection, typer, thinc, langsmith, weasel, langchain-core, spacy, langchain-text-splitters, langchain\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 25.0\n",
      "    Uninstalling packaging-25.0:\n",
      "      Successfully uninstalled packaging-25.0\n",
      "Successfully installed PyYAML-6.0.2 SQLAlchemy-2.0.41 annotated-types-0.7.0 anyio-4.9.0 async-timeout-4.0.3 blis-1.3.0 catalogue-2.0.10 click-8.2.1 cloudpathlib-0.21.1 confection-0.1.5 cymem-2.0.11 greenlet-3.2.3 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 joblib-1.5.1 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.26 langchain-core-0.3.66 langchain-text-splitters-0.3.8 langcodes-3.5.0 langsmith-0.4.2 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.13 nltk-3.9.1 orjson-3.10.18 packaging-24.2 pinecone-client-6.0.0 pinecone-plugin-interface-0.0.7 preshed-3.0.10 pydantic-2.11.7 pydantic-core-2.33.2 python-dotenv-1.1.1 requests-toolbelt-1.0.0 rich-14.0.0 shellingham-1.5.4 smart-open-7.1.0 sniffio-1.3.1 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 tenacity-9.1.2 thinc-8.3.6 typer-0.16.0 typing-inspection-0.4.1 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.2 zstandard-0.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script dotenv.exe is installed in 'c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script nltk.exe is installed in 'c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script markdown-it.exe is installed in 'c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script httpx.exe is installed in 'c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script typer.exe is installed in 'c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script weasel.exe is installed in 'c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script spacy.exe is installed in 'c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1.1\n",
      "[notice] To update, run: c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain spacy nltk pinecone-client python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbbd88e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: pip in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (23.0.1)\n",
      "Collecting pip\n",
      "  Using cached pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.0.1\n",
      "    Uninstalling pip-23.0.1:\n",
      "      Successfully uninstalled pip-23.0.1\n",
      "Successfully installed pip-25.1.1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts pip.exe, pip3.10.exe and pip3.exe are installed in 'c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5f15138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from langchain.schema import Document\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cd045d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mercy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mercy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8612cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number</th>\n",
       "      <th>Youtube_link</th>\n",
       "      <th>Subject</th>\n",
       "      <th>title</th>\n",
       "      <th>channel</th>\n",
       "      <th>description</th>\n",
       "      <th>length</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>views</th>\n",
       "      <th>error</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.youtube.com/watch?v=tOaMRG8DX3U</td>\n",
       "      <td>An AI Agent that knows everything about your P...</td>\n",
       "      <td>An AI Agent that knows everything about your P...</td>\n",
       "      <td>ServiceNow Community</td>\n",
       "      <td>Supercharge Your Project Management with AI!\\n...</td>\n",
       "      <td>738.0</td>\n",
       "      <td>20250221.0</td>\n",
       "      <td>1063.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hey folks, how you doing Chris Thanky here? A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://www.youtube.com/watch?v=vteLoWpNw8Q</td>\n",
       "      <td>What Is Agentic AI and Why Should I Care?</td>\n",
       "      <td>What Is Agentic AI and Why Should I Care?</td>\n",
       "      <td>ServiceNow Community</td>\n",
       "      <td>In today’s solo-Lauren episode of Break Point,...</td>\n",
       "      <td>1158.0</td>\n",
       "      <td>20250124.0</td>\n",
       "      <td>1936.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In today's Solo Lauren episode of the Breakpo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://www.youtube.com/watch?v=7WJ6lmxa1WQ</td>\n",
       "      <td>Agentic AI workflows for AIOps</td>\n",
       "      <td>Agentic AI workflows for AIOps</td>\n",
       "      <td>ServiceNow Community</td>\n",
       "      <td>In this episode, we explore the transformative...</td>\n",
       "      <td>995.0</td>\n",
       "      <td>20250518.0</td>\n",
       "      <td>395.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hello, everyone. Welcome to another episode o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://www.youtube.com/watch?v=fqB-NcZmqXo</td>\n",
       "      <td>ServiceNow's agentic AI framework explained: W...</td>\n",
       "      <td>ServiceNow's agentic AI framework explained: W...</td>\n",
       "      <td>ServiceNow Community</td>\n",
       "      <td>In this episode, we explore how ServiceNow’s A...</td>\n",
       "      <td>946.0</td>\n",
       "      <td>20250312.0</td>\n",
       "      <td>4048.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hello everyone, welcome to another episode of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://www.youtube.com/watch?v=ZYJqkxGrNiI</td>\n",
       "      <td>AI and Business Agility: Enhancing Human Intel...</td>\n",
       "      <td>AI and Business Agility: Enhancing Human Intel...</td>\n",
       "      <td>ServiceNow Community</td>\n",
       "      <td>In this episode of the ServiceNow Enterprise A...</td>\n",
       "      <td>730.0</td>\n",
       "      <td>20241022.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Welcome everyone to another edition of the Se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number                                 Youtube_link  \\\n",
       "0       1  https://www.youtube.com/watch?v=tOaMRG8DX3U   \n",
       "1       2  https://www.youtube.com/watch?v=vteLoWpNw8Q   \n",
       "2       3  https://www.youtube.com/watch?v=7WJ6lmxa1WQ   \n",
       "3       4  https://www.youtube.com/watch?v=fqB-NcZmqXo   \n",
       "4       5  https://www.youtube.com/watch?v=ZYJqkxGrNiI   \n",
       "\n",
       "                                             Subject  \\\n",
       "0  An AI Agent that knows everything about your P...   \n",
       "1          What Is Agentic AI and Why Should I Care?   \n",
       "2                     Agentic AI workflows for AIOps   \n",
       "3  ServiceNow's agentic AI framework explained: W...   \n",
       "4  AI and Business Agility: Enhancing Human Intel...   \n",
       "\n",
       "                                               title               channel  \\\n",
       "0  An AI Agent that knows everything about your P...  ServiceNow Community   \n",
       "1          What Is Agentic AI and Why Should I Care?  ServiceNow Community   \n",
       "2                     Agentic AI workflows for AIOps  ServiceNow Community   \n",
       "3  ServiceNow's agentic AI framework explained: W...  ServiceNow Community   \n",
       "4  AI and Business Agility: Enhancing Human Intel...  ServiceNow Community   \n",
       "\n",
       "                                         description  length  publish_date  \\\n",
       "0  Supercharge Your Project Management with AI!\\n...   738.0    20250221.0   \n",
       "1  In today’s solo-Lauren episode of Break Point,...  1158.0    20250124.0   \n",
       "2  In this episode, we explore the transformative...   995.0    20250518.0   \n",
       "3  In this episode, we explore how ServiceNow’s A...   946.0    20250312.0   \n",
       "4  In this episode of the ServiceNow Enterprise A...   730.0    20241022.0   \n",
       "\n",
       "    views error                                         transcript  \n",
       "0  1063.0   NaN   Hey folks, how you doing Chris Thanky here? A...  \n",
       "1  1936.0   NaN   In today's Solo Lauren episode of the Breakpo...  \n",
       "2   395.0   NaN   Hello, everyone. Welcome to another episode o...  \n",
       "3  4048.0   NaN   Hello everyone, welcome to another episode of...  \n",
       "4    60.0   NaN   Welcome everyone to another edition of the Se...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_transcripts = pd.read_csv(\"Data/video_metadata_with_transcripts.csv\")\n",
    "df_transcripts.columns\n",
    "df_transcripts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6decc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"\\s+\", \" \", text)        # Remove extra whitespace\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)     # Remove punctuation\n",
    "    return text.lower().strip()\n",
    "\n",
    "def tokenize_and_lemmatize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "def merge_metadata_transcripts(metadata_path, transcript_path):\n",
    "    df_meta = pd.read_csv(metadata_path)\n",
    "    df_trans = pd.read_csv(transcript_path)\n",
    "     # Merge on 'Youtube_link' (or rename to match)\n",
    "    df = pd.merge(df_meta, df_trans, on=\"Youtube_link\", how=\"inner\")\n",
    "    df[\"cleaned_transcript\"] = df[\"Transcript\"].apply(clean_text).apply(tokenize_and_lemmatize)\n",
    "    return df\n",
    "\n",
    "def prepare_langchain_docs(df):\n",
    "    docs = []\n",
    "    for _, row in df.iterrows():\n",
    "        metadata = {\n",
    "            \"title\": row[\"title\"],\n",
    "            \"url\": row[\"Youtube_link\"],\n",
    "            \"subject\": row[\"Subject\"]\n",
    "        }\n",
    "        doc = Document(page_content=row[\"cleaned_transcript\"], metadata=metadata)\n",
    "        docs.append(doc)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47e4f196",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Transcript'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Transcript'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m transcript_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData/video_metadata_with_transcripts.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Step 1: Merge and preprocess\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m df_clean \u001b[38;5;241m=\u001b[39m \u001b[43mmerge_metadata_transcripts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranscript_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Step 2: Convert to LangChain docs\u001b[39;00m\n\u001b[0;32m      9\u001b[0m langchain_docs \u001b[38;5;241m=\u001b[39m prepare_langchain_docs(df_clean)\n",
      "Cell \u001b[1;32mIn[20], line 18\u001b[0m, in \u001b[0;36mmerge_metadata_transcripts\u001b[1;34m(metadata_path, transcript_path)\u001b[0m\n\u001b[0;32m     16\u001b[0m  \u001b[38;5;66;03m# Merge on 'Youtube_link' (or rename to match)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(df_meta, df_trans, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYoutube_link\u001b[39m\u001b[38;5;124m\"\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcleaned_transcript\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTranscript\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(clean_text)\u001b[38;5;241m.\u001b[39mapply(tokenize_and_lemmatize)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mc:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4107\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4109\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3817\u001b[0m     ):\n\u001b[0;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Transcript'"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "metadata_file = \"Data/ServiceNow_Youtube_Metadata_Clean.csv\"\n",
    "transcript_file = \"Data/video_metadata_with_transcripts.csv\"\n",
    "\n",
    "# Step 1: Merge and preprocess\n",
    "df_clean = merge_metadata_transcripts(metadata_file, transcript_file)\n",
    "\n",
    "# Step 2: Convert to LangChain docs\n",
    "langchain_docs = prepare_langchain_docs(df_clean)\n",
    "\n",
    "# Optional: Print sample\n",
    "print(langchain_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3986f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "Document(\n",
    "    page_content=\"this is lemmatized transcript text\",\n",
    "    metadata={'title': 'AI and Business Agility', 'url': 'https://...', 'subject': 'AI for Business'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9153fc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 18:50:53,834 - INFO - Saved 327 transcript chunks to Data/processed_transcripts.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 327 transcript chunks to Data/processed_transcripts.csv\n"
     ]
    }
   ],
   "source": [
    "# Save processed data\n",
    "output_file = \"Data/processed_transcripts.csv\"\n",
    "try:\n",
    "    df_processed = pd.DataFrame(processed_data)\n",
    "    df_processed.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    logging.info(f\"Saved {len(df_processed)} transcript chunks to {output_file}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error saving processed data: {e}\")\n",
    "    raise\n",
    "\n",
    "# Log missing transcripts\n",
    "if missing_transcripts:\n",
    "    logging.warning(f\"Missing transcripts for video IDs: {missing_transcripts}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"Processed {len(df_processed)} transcript chunks to {output_file}\")\n",
    "if missing_transcripts:\n",
    "    print(f\"Missing transcripts for {len(missing_transcripts)} videos: {missing_transcripts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "106cd701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   video_id chunk_id                                               text  \\\n",
      "0         1      1_0  hey folk Chris Thanky like welcome session pla...   \n",
      "1         1      1_1  info customer comment say project manager come...   \n",
      "2         1      1_2  like practice give example mention customer wa...   \n",
      "3         1      1_3  send information interpret aggregate pull know...   \n",
      "4         1      1_4  answer go auto button LLM start tell look tell...   \n",
      "\n",
      "                                             subject  \n",
      "0  An AI Agent that knows everything about your P...  \n",
      "1  An AI Agent that knows everything about your P...  \n",
      "2  An AI Agent that knows everything about your P...  \n",
      "3  An AI Agent that knows everything about your P...  \n",
      "4  An AI Agent that knows everything about your P...  \n"
     ]
    }
   ],
   "source": [
    "df_processed = pd.read_csv(\"data/processed_transcripts.csv\")\n",
    "print(df_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8cbf62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing transcripts for video IDs: []\n"
     ]
    }
   ],
   "source": [
    "missing = df[df[\"transcript\"].isna()][\"Number\"].tolist()\n",
    "print(f\"Missing transcripts for video IDs: {missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb7e87e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation report saved to docs/validation_report.txt\n",
      "Metadata success: 21/22\n",
      "Missing transcripts: 0/22\n",
      "Processed chunks: 327\n"
     ]
    }
   ],
   "source": [
    "df_metadata = pd.read_csv(\"data/ServiceNow_Youtube_Metadata_Clean.csv\")\n",
    "df_transcripts = pd.read_csv(\"data/video_metadata_with_transcripts.csv\")\n",
    "df_processed = pd.read_csv(\"data/processed_transcripts.csv\")\n",
    "\n",
    "# Validation checks\n",
    "metadata_success = df_metadata[\"title\"].notnull().sum()\n",
    "missing_transcripts = df_transcripts[\"transcript\"].isna().sum()\n",
    "total_chunks = len(df_processed)\n",
    "sample_chunk = df_processed[\"text\"].iloc[0] if not df_processed.empty else \"No chunks\"\n",
    "\n",
    "import os\n",
    "\n",
    "# Ensure the 'docs' directory exists\n",
    "os.makedirs(\"docs\", exist_ok=True)\n",
    "\n",
    "# Save report\n",
    "with open(\"docs/validation_report.txt\", \"w\") as f:\n",
    "    f.write(f\"Metadata: {len(df_metadata)} videos, {metadata_success} with valid metadata\\n\")\n",
    "    f.write(f\"Transcripts: {len(df_transcripts)} videos, {missing_transcripts} missing\\n\")\n",
    "    f.write(f\"Processed Chunks: {total_chunks}\\n\")\n",
    "    f.write(f\"Sample Chunk:\\n{sample_chunk}\\n\")\n",
    "print(\"Validation report saved to docs/validation_report.txt\")\n",
    "\n",
    "# Print summary\n",
    "print(f\"Metadata success: {metadata_success}/{len(df_metadata)}\")\n",
    "print(f\"Missing transcripts: {missing_transcripts}/{len(df_transcripts)}\")\n",
    "print(f\"Processed chunks: {total_chunks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "277c1e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir src"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
