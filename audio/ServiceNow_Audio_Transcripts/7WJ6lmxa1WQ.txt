 Hello, everyone. Welcome to another episode of our podcast. Today, we're going to dive again into an eGintiq AI framework. I have another technology leader with me, Rani Zures, joins us from Israel. Rani, welcome. How are you? Good. Good. Thank you. How are you doing? Yes, great. We're going to dive into awesome stuff today. He's the leader building products for AIOps for a long time, bringing a lot of new innovations. I'm really excited to have him on this podcast. Before we dive in, Rani, tell me a little bit about, do you use GenaAI in your world? Of course, you do. You're making new products, but anything interesting that you do with it? Every day, day in, day out, all in our, both exploring different technologies in GenaAI and personally using. It's really, help me to arrange my thoughts and put together my thoughts into structure ways to make sure that the messages I'm trying to convey are clear and concise. It's super helpful for me. Yes, that's great. I started to use a little bit with my kids to make different images, try to understand their imagination, what they can imagine, and how GenaAI tools can help them create something that they are thinking about. Let's dive right in. I want to give you a bit of deep knowledge of machine learning and AI in a traditional way and now shifting towards agentic frameworks. How does in your world, agentic AI frameworks differ from a traditional AI machine learning frameworks? Yes. The thing is that when you look at traditional AI and ML frameworks, mainly focus on data processing and pattern recognition, and they provide insights and predictions and recommendations, but really require human intervention for the execution. When you look on agentic AI, on the other hand, it introduces a lot of autonomous and decision making entities that actually understand the environment, which is super critical in our space to understand exactly different types of data. It's also have reasoning and planning capabilities to provide solution, which is based on either historical data or real-time context, and it's able to act on behalf of people. It could be your own co-pilot or your own little minion that helps you do. It also has the ability to learn and adapt like any other ML and AI from user feedback and continuously refining their decision. This end-to-end actually make the agentic AI far more dynamic and action-oriented than just traditional AI models which are focused on insights and predictions. You're the second person who said the word co-pilot this week in the podcast. I had another customer actually say that. I think it is a concept of, as you said, using it to create actions is really what's becoming very useful in a daily life, but also for our customers. Let's dive a little bit into the world we live in, that is, IT operations. How are you seeing agentic AI enhance the IT operations and maybe some examples of what you're working on coming up in the near future? If you look on agating AI, this is actually going to transform how IT operations are working today. It's going to be more focusing on automatic decision-making and execution. It will significantly reduce the manual burden on IT teams. Unlike traditional AI, which is primarily analyzed data and provides insights, the AI agents can autonomously act on alerts and prioritize tasks and even resolve issues, all while keeping operations in control. For the IT operations, this means faster resolution because AI agents will triage, correlate, and assign tasks in real time. You can imagine proactive issue prevention because the AI will be in the position to detect issues that indicate potential failure, potential outages, and recommend on execute preventive actions. It will also reduce the operation load because AI agents will be able to handle all the repetitive troubleshooting tasks and letting the operator focus on high-value problems rather than just yet another task that they have to perform. It's going to empower our operators to be much more focused on high-value or critical issues. Yes. I guess when we speak in the terms of autonomous and again, I guess we think about various ways. One is autonomous, assisted, human, in the loop, human, on the loop. All these are different ways customers and enterprise start to adopt it. Do you think about a maturity based on where our customers are and how would you baseline that maturity if they want to move to an agente framework? When we think about how our users or our customers will adopt this kind of a technology, what we imagine is how the agente AI actually takes responsibility on pieces of the entire user journey of our users. If our users are, let's say, operators or any technology owner, they have their user journey. They are facing alerts and they have to try out the alert, they have to investigate them, they have to find the root cause analysis of them, they have to remediate the issue, they have to escalate issues, they have to create all sorts of tasks or records to other people. All of these tasks are parts of the user journey they are doing on their day to day and we can imagine AI agents take full control over pieces of that journey, but taking full control, not in the matter of replacing the user, but actually empowering the users because the agent will be able to collect the data, analyze the data and bring the bottom line and the decision to the user to take the decision. So the user can decide or let the agent do everything automatically or decide to be the one that is taking the decision for each and every step. So it's kind of saving the time of our users from doing the repetitive tasks and doing the deep level investigation and just provide the bottom line to the user and ask them for their approval or the decision so the agent can continuously do the data task for them in an automatic way. What our CEO Bill McDermott says is we are building agents in the service of humans. So I think that says a lot about the maturity of our customers as they adopt it. One question that comes up as we are experimenting with different large language models is there are so many out there now and there are benchmarks, are they right? Sometimes the entropy clot comes out really well on an encoding task or ask. DeepSeq actually brought in very good reasoning techniques and we have the GPT models and the mini models doing really good reasoning. So in your world as you're developing new products, how do you see these different models come together to solve specific problems or use cases? Yeah, very interesting. So what we can see is AI agents that can collaborate across multiple LLMs. So you have multiple agents that are able to collaborate and converse between them. Each AI agent is a master of a specific task or a decision. Each one of them can actually use a specialized LLM to handle different tasks. So the AI agents, they can call APIs, they can interact with observability tools, they can communicate with other agents. At the end of the day, each one of them is specialized in a certain task. So each one of them can work across different LLMs. So the kind of approach that we are taking is that AI agents are integrating across different products, across different domains that are used different LLMs according to their specialized model. And this goal is actually maximizing the efficiency while ensuring seamless collaboration between agents and users. Cool. Let's drive down an example maybe in the operations world. As you said before, triaging is really important. Outage deduction time to detect an outage and then remediate that outage is really important for all the IT operations leaders. So if you think about that scenario that we live in day in, day out, how would an agent take framework help that? Yeah. So let's talk about first about our user. Let's imagine an operator that is bombed with dozens or hundreds of alerts a day covering different technologies, different services and applications. They have a real trouble to understand exactly what is in priority, what is the impact of the issues, does it even matter to anyone? And then after realizing the priority of it, they have to understand the impact of it and what exactly is the issue and how to remediate that. And now imagine that these operators are not always have access to all the different observability tools and they they're not necessarily the one expert in a certain technology. So the AI agents by using different AI agents and integrate and integrating also with potentially external AI vendors or external observability tools with their AI were able to using these AI agents to process all this information and provide the operator the bottom line of it, which means what is the issue, what is the impact of the issue, who is impacted by the issue and what is usually being done in such cases. Now you have different AIs that are collaborating with each other each is providing their own insight into it and then we can actually bring the bottom line plus the action to the user. For example, an action could be do you want me to create an incident for you dear user or do you want me just to close this alert because it's not that in fact impactful or do you want me to run an automation which is pre-made in our system for you. So the thing is that instead of the user going search and find and ask for people to access certain tools, the AI agents will do that for them and will bring the insight plus the action to perform instead of them. So imagine that the operator instead of just looking on a bunch of alerts, they will look only on the high impacting one, the one that requires manual intervention, the one that requires some level of escalation and all the repetitive tasks will be done for them. Yeah, one thing that you mentioned is so crucial as the enterprises use service now platform is ability for them to harness the real time and the historical data that is present already in the platform as you mentioned, incidents, changes, problems and knowledge that is already available to them. The ability for us to provide that real time monitoring and you know and cobble that together all of it in a very seamless fashion and as we building new data fabrics and ability to bring rapid DP. So we have multiple sets of data right? I mean in a lot of times we are looking at monitoring discovery data but then there are other sets of data maybe present in data links that may be able to help with this reoging right? I mean do you think that could also happen in the future? Definitely, you know you'll have specialized agent in for example finding the right knowledge base article instead of the user going and search for knowledge base articles reading tons of them just to understand which one is the one relevant. You'll have an agent do that for them and just will provide them the bottom line and summarize kind of a summary of the relevant KB articles with the exact steps. It could be also an external body of knowledge, it could be an external website which is coming with the industry knowledge, it could be something which we are passing from the resolution notes of past incidents or some relevant changes that happened recently or for something from observability tools. All of that you'll have different agents providing all these insights of course not each one of them is relevant for each and every issue everything will be orchestrated so only the most meaningful and the bottom line will be surface to the user with the actual outcome instead of you know instead of taking data from all over the places it's just bomb the user with all with tons of data we will just process everything and and provide him the bottom line of it. Sure, a lot to unpack here and you're working diligently on this new development so you know we'll bring you back here, hear a lot more when these agents are released. Anything you want to tell our customers and who are looking to get into this space in terms of how they can get involved or any other things that we you know any outcomes that we didn't talk about that you want to say? Yeah, so a lot of times I'm being asked like okay what why should I do that? What would be the outcome? Why should I invest in adopting AI into my organization? Because it's a it's a big change right we need to train our users to now interact with the AI to understand exactly recommendations that are coming with AI to interact with AI to give to provide feedback for AI. There's a lot of a lot of adoption exercise that has to occur with our customers and what we are telling our customers is that we need to be focused on the outcomes. What are we going to what are you going to gain by using it by using it and it's always boils down to the standard KPIs like increasing the productivity of your users, the manual burden of different you know different tasks, being much more proactive and predictive operations and obviously to lower the MTTR and maybe eliminate different outages. With this capability imagine that instead of having a group of 10 operators you're actually going to multiply that by the number of agents that are going to work and you're not going to hire more and more people to perform more and more tasks you're just going to delegate a lot to the AI and then your human users will be the one that are going to supervise the AI. Imagine that the new operator will be the one to supervise on AI agents so instead of being hammered by you know different alerts they will be the one to make sure that the AI is doing the work for them and he will be focusing on the meaningful work so you're going to multiply your operations just by delegating more and more capabilities to AI. Awesome and we will leave your contact information as well in the podcast so people can reach out and I think a plug for everyone is if you want to get involved you can be an early adopter program. The things that we have done since last year in the IT operations management product helps you accelerate the adoption rate so you don't have to wait for a certain you know CMDB population or generally if you're trying to look at a data hygiene you don't have to wait right so for all those things you can adopt these capabilities quickly experiment and test them right away in matter of weeks without waiting for a longer adoption cycle and then if you have a high rate data you want to bring it in as I said we have not only the monitoring capabilities to inject that information but also the data fabric to bring in various data stores. With that Ronnie thanks for being here I know it's very late for you so thanks for being a great sport always a pleasure and let's get you back here soon but thanks for coming on board and thanks for everyone listening. Thank you. See you.