{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84c9e2a5",
   "metadata": {},
   "source": [
    "## Model: distilgpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3ab7901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "from pinecone import Pinecone\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d77e64",
   "metadata": {},
   "source": [
    "verify Pinecone integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29ee1f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 17:53:16,283 - INFO - Total vectors in index youtube-transcripts: 615\n",
      "2025-06-26 17:53:16,284 - INFO - Verification successful: 615 vectors match expected count 615\n",
      "2025-06-26 17:53:16,583 - INFO - Found vector for chunk_id 1_0\n",
      "2025-06-26 17:53:16,726 - INFO - Found vector for chunk_id 2_0\n",
      "2025-06-26 17:53:16,845 - INFO - Found vector for chunk_id 3_0\n",
      "2025-06-26 17:53:17,000 - INFO - Found vector for chunk_id 4_0\n",
      "2025-06-26 17:53:17,122 - INFO - Found vector for chunk_id 5_0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "615"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def verify_pinecone_vectors(index_name='youtube-transcripts', expected_count=615):\n",
    "    \"\"\"Verify vectors in Pinecone index.\"\"\"\n",
    "    load_dotenv()\n",
    "    pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "    if not pinecone_api_key:\n",
    "        logging.error('PINECONE_API_KEY not found in .env')\n",
    "        raise ValueError('PINECONE_API_KEY not found')\n",
    "\n",
    "    # Initialize Pinecone\n",
    "    pc = Pinecone(api_key=pinecone_api_key)\n",
    "    if index_name not in [idx['name'] for idx in pc.list_indexes()]:\n",
    "        logging.error(f'Index {index_name} not found')\n",
    "        raise ValueError(f'Index {index_name} not found')\n",
    "\n",
    "    # Connect to index\n",
    "    index = pc.Index(index_name)\n",
    "    \n",
    "    # Get index stats\n",
    "    stats = index.describe_index_stats()\n",
    "    total_vectors = stats['total_vector_count']\n",
    "    \n",
    "    # Verify vector count\n",
    "    logging.info(f'Total vectors in index {index_name}: {total_vectors}')\n",
    "    if total_vectors == expected_count:\n",
    "        logging.info(f'Verification successful: {total_vectors} vectors match expected count {expected_count}')\n",
    "    else:\n",
    "        logging.warning(f'Verification failed: Found {total_vectors} vectors, expected {expected_count}')\n",
    "\n",
    "    # Sample a few vector IDs to confirm format\n",
    "    sample_ids = [f'{i}_0' for i in range(1, 6)]  # Check first chunk of videos 1-5\n",
    "    for sample_id in sample_ids:\n",
    "        try:\n",
    "            result = index.fetch(ids=[sample_id])\n",
    "            if sample_id in result['vectors']:\n",
    "                logging.info(f'Found vector for chunk_id {sample_id}')\n",
    "            else:\n",
    "                logging.warning(f'No vector found for chunk_id {sample_id}')\n",
    "        except Exception as e:\n",
    "            logging.error(f'Error fetching chunk_id {sample_id}: {e}')\n",
    "\n",
    "    return total_vectors\n",
    "\n",
    "# Run verification\n",
    "verify_pinecone_vectors()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3063b6af",
   "metadata": {},
   "source": [
    "Implement Retrieval-Augmented Generation (RAG) Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98469973",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def retrieve_chunks(query, index_name='youtube-transcripts', top_k=5):\n",
    "    \"\"\"Retrieve top-k transcript chunks from Pinecone.\"\"\"\n",
    "    load_dotenv()\n",
    "    pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "    if not pinecone_api_key:\n",
    "        logging.error('PINECONE_API_KEY not found in .env')\n",
    "        raise ValueError('PINECONE_API_KEY not found')\n",
    "\n",
    "    # Initialize Pinecone and SentenceTransformer\n",
    "    pc = Pinecone(api_key=pinecone_api_key)\n",
    "    index = pc.Index(index_name)\n",
    "    embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    # Encode query\n",
    "    query_embedding = embedder.encode(query, show_progress_bar=False).tolist()\n",
    "\n",
    "    # Query Pinecone\n",
    "    results = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n",
    "    chunks = [match['metadata']['text'] for match in results['matches']]\n",
    "    \n",
    "    logging.info(f'Retrieved {len(chunks)} chunks for query: {query}')\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c11a0843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b3e8efb1a944f7a1088497203b3692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Mercy\\.cache\\huggingface\\hub\\models--google--flan-t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d15d50d56749cb90c5aa8707394a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda23fae9d534018be49749322028f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b61f72f9054eb4b0f0398592c0c601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47483106f9df4d53971bd4054da07910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6228627d43b4198b17cd99a9ceb2bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fcea8b2c5e24c9fa5f81ea82136d6b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ensure latest transformers and huggingface_hub are installed\n",
    "#%pip install --upgrade transformers huggingface_hub\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "# Load Mistral model and tokenizer\n",
    "#import torch\n",
    "#os.environ[\"USE_TF\"] = \"0\" \n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d4b5961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, chunks):\n",
    "    context = \"\\n\".join(chunks) if chunks else \"No relevant information found.\"\n",
    "\n",
    "    prompt = f\"\"\"[INST] You are a helpful ServiceNow expert. Based on the following context, answer the query accurately and concisely.\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Query: {query}\n",
    "Answer: [/INST]\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "        repetition_penalty=1.2\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract response after \"Answer:\" if needed\n",
    "    return response.split(\"Answer:\")[-1].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3266de90",
   "metadata": {},
   "source": [
    "Built Chatbot interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98557afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 17:55:52,142 - INFO - Use pytorch device_name: cpu\n",
      "2025-06-26 17:55:52,145 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-06-26 17:55:56,131 - INFO - Retrieved 5 chunks for query: What is ITSM in ServiceNow?\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (845 > 512). Running this sequence through the model will result in indexing errors\n",
      "2025-06-26 17:55:58,049 - INFO - Use pytorch device_name: cpu\n",
      "2025-06-26 17:55:58,049 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is ITSM in ServiceNow?\n",
      "A: aiops and event management\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 17:56:00,548 - INFO - Retrieved 5 chunks for query: Explain CMDB relationships.\n",
      "2025-06-26 17:56:01,867 - INFO - Use pytorch device_name: cpu\n",
      "2025-06-26 17:56:01,868 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Explain CMDB relationships.\n",
      "A: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 17:56:04,634 - INFO - Retrieved 5 chunks for query: How does Incident Management work?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: How does Incident Management work?\n",
      "A: The key is being proactive versus reactive and having the right supporting system to be able to achieve that\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_queries = [\n",
    "    \"What is ITSM in ServiceNow?\",\n",
    "    \"Explain CMDB relationships.\",\n",
    "    \"How does Incident Management work?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    chunks = retrieve_chunks(query)\n",
    "    result = generate_response(query, chunks)\n",
    "    print(f\"Q: {query}\\nA: {result}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
