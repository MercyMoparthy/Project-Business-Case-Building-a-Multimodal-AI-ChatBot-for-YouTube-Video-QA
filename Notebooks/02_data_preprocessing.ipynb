{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca79a96b",
   "metadata": {},
   "source": [
    "## Data PreProcessing\n",
    "\n",
    "* This notebook preprocesses YouTube video transcripts for the multimodal AI chatbot project. \n",
    "* It loads metadata and transcripts, cleans and lemmatizes text, splits into chunks using LangChain, and saves processed data for vector database integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ab8c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain spacy nltk python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbbd88e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4437350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y pinecone-client --quiet\n",
    "%pip install pinecone-client==3.0.0 --quiet\n",
    "%pip install -U langchain --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5f15138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_text_splitters.character import RecursiveCharacterTextSplitter\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cd045d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8612cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_file = 'data/ServiceNow_Youtube_Metadata_Clean.csv'\n",
    "transcript_file = 'data/video_metadata_with_transcripts.csv'\n",
    "output_file = 'data/processed_transcripts.csv'\n",
    "validation_report = 'docs/validation_report.txt'\n",
    "\n",
    "for file in [metadata_file, transcript_file]:\n",
    "    if not os.path.exists(file):\n",
    "        logging.error(f'File not found: {file}')\n",
    "        raise FileNotFoundError(f'File not found: {file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a67f5094",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('docs', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b38eb36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata columns: ['Number', 'Youtube_link', 'Subject', 'title', 'channel', 'description', 'length', 'publish_date', 'views', 'error']\n",
      "Transcript columns: ['Number', 'Youtube_link', 'Subject', 'title', 'channel', 'description', 'length', 'publish_date', 'views', 'error', 'transcript']\n"
     ]
    }
   ],
   "source": [
    "df_meta = pd.read_csv('data/ServiceNow_Youtube_Metadata_Clean.csv')\n",
    "print(\"Metadata columns:\", df_meta.columns.tolist())\n",
    "\n",
    "df_trans = pd.read_csv('data/video_metadata_with_transcripts.csv')\n",
    "print(\"Transcript columns:\", df_trans.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10693cd",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "* Lemmatization & Tokenization\n",
    "* Normalizing the columns\n",
    "* Splitting the Chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6decc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 20:46:30,137 - INFO - Loaded and processed 22 transcript records\n",
      "2025-06-26 20:46:30,141 - INFO - Created 14 chunks for video_id 1\n",
      "2025-06-26 20:46:30,147 - INFO - Created 21 chunks for video_id 2\n",
      "2025-06-26 20:46:30,150 - INFO - Created 17 chunks for video_id 3\n",
      "2025-06-26 20:46:30,154 - INFO - Created 17 chunks for video_id 4\n",
      "2025-06-26 20:46:30,157 - INFO - Created 11 chunks for video_id 5\n",
      "2025-06-26 20:46:30,164 - INFO - Created 14 chunks for video_id 6\n",
      "2025-06-26 20:46:30,169 - INFO - Created 17 chunks for video_id 7\n",
      "2025-06-26 20:46:30,172 - INFO - Created 8 chunks for video_id 8\n",
      "2025-06-26 20:46:30,181 - INFO - Created 15 chunks for video_id 9\n",
      "2025-06-26 20:46:30,186 - INFO - Created 21 chunks for video_id 10\n",
      "2025-06-26 20:46:30,193 - INFO - Created 22 chunks for video_id 11\n",
      "2025-06-26 20:46:30,195 - INFO - Created 5 chunks for video_id 12\n",
      "2025-06-26 20:46:30,200 - INFO - Created 14 chunks for video_id 13\n",
      "2025-06-26 20:46:30,204 - INFO - Created 21 chunks for video_id 14\n",
      "2025-06-26 20:46:30,207 - INFO - Created 13 chunks for video_id 15\n",
      "2025-06-26 20:46:30,213 - INFO - Created 17 chunks for video_id 16\n",
      "2025-06-26 20:46:30,217 - INFO - Created 31 chunks for video_id 17\n",
      "2025-06-26 20:46:30,221 - INFO - Created 10 chunks for video_id 18\n",
      "2025-06-26 20:46:30,224 - INFO - Created 8 chunks for video_id 19\n",
      "2025-06-26 20:46:30,227 - INFO - Created 6 chunks for video_id 20\n",
      "2025-06-26 20:46:30,233 - INFO - Created 25 chunks for video_id 21\n",
      "2025-06-26 20:46:30,234 - INFO - Created 1 chunks for video_id 22\n",
      "2025-06-26 20:46:30,234 - INFO - Created 328 total LangChain documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 328 document chunks.\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        logging.warning('Empty text encountered in clean_text')\n",
    "        return ''\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text.lower().strip()\n",
    "\n",
    "def tokenize_and_lemmatize(text):\n",
    "    if not text:\n",
    "        logging.warning('Empty text encountered in tokenize_and_lemmatize')\n",
    "        return ''\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens if token]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "def load_and_process_transcripts(transcript_path):\n",
    "    df = pd.read_csv(transcript_path)\n",
    "    \n",
    "    # Normalize columns (make sure names are lowercase and consistent)\n",
    "    df = df.rename(columns={\n",
    "        'Youtube_link': 'youtube_link',\n",
    "        'Number': 'video_id',\n",
    "        'Subject': 'subject',\n",
    "        'title': 'title',\n",
    "        'transcript': 'transcript'\n",
    "    })\n",
    "    \n",
    "    # Clean and lemmatize transcript text\n",
    "    df['cleaned_transcript'] = df['transcript'].apply(clean_text).apply(tokenize_and_lemmatize)\n",
    "    \n",
    "    logging.info(f'Loaded and processed {len(df)} transcript records')\n",
    "    return df\n",
    "\n",
    "def prepare_langchain_docs(df):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "    docs = []\n",
    "    missing_transcripts = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        if not row['cleaned_transcript']:\n",
    "            missing_transcripts.append(row['video_id'])\n",
    "            logging.warning(f'Missing transcript for video_id {row[\"video_id\"]}')\n",
    "            continue\n",
    "        \n",
    "        metadata = {\n",
    "            'title': row['title'],\n",
    "            'url': row['youtube_link'],\n",
    "            'subject': row['subject'],\n",
    "            'video_id': row['video_id'],\n",
    "        }\n",
    "        \n",
    "        chunks = text_splitter.split_text(row['cleaned_transcript'])\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            doc = Document(page_content=chunk, metadata={**metadata, 'chunk_id': f\"{row['video_id']}_{i}\"})\n",
    "            docs.append(doc)\n",
    "            \n",
    "        logging.info(f'Created {len(chunks)} chunks for video_id {row[\"video_id\"]}')\n",
    "        \n",
    "    logging.info(f'Created {len(docs)} total LangChain documents')\n",
    "    return docs, missing_transcripts\n",
    "\n",
    "# Usage example:\n",
    "df_clean = load_and_process_transcripts(transcript_file)\n",
    "langchain_docs, missing = prepare_langchain_docs(df_clean)\n",
    "\n",
    "print(f\"Processed {len(langchain_docs)} document chunks.\")\n",
    "if missing:\n",
    "    print(f\"Missing transcripts for video IDs: {missing}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52485296",
   "metadata": {},
   "source": [
    "## Preprocess Transcripts\n",
    "* Loading and Preprocessing transcripts\n",
    "* Converting to LangChain docs and Split Chunks\n",
    "* Saving Processed Data as processed_transcripts.csv file\n",
    "* Validating creating Data validation file as validation_report.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1193125e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 20:46:30,564 - INFO - Loaded and processed 22 transcript records\n",
      "2025-06-26 20:46:30,568 - INFO - Created 14 chunks for video_id 1\n",
      "2025-06-26 20:46:30,571 - INFO - Created 21 chunks for video_id 2\n",
      "2025-06-26 20:46:30,577 - INFO - Created 17 chunks for video_id 3\n",
      "2025-06-26 20:46:30,585 - INFO - Created 17 chunks for video_id 4\n",
      "2025-06-26 20:46:30,589 - INFO - Created 11 chunks for video_id 5\n",
      "2025-06-26 20:46:30,596 - INFO - Created 14 chunks for video_id 6\n",
      "2025-06-26 20:46:30,602 - INFO - Created 17 chunks for video_id 7\n",
      "2025-06-26 20:46:30,603 - INFO - Created 8 chunks for video_id 8\n",
      "2025-06-26 20:46:30,608 - INFO - Created 15 chunks for video_id 9\n",
      "2025-06-26 20:46:30,615 - INFO - Created 21 chunks for video_id 10\n",
      "2025-06-26 20:46:30,620 - INFO - Created 22 chunks for video_id 11\n",
      "2025-06-26 20:46:30,622 - INFO - Created 5 chunks for video_id 12\n",
      "2025-06-26 20:46:30,628 - INFO - Created 14 chunks for video_id 13\n",
      "2025-06-26 20:46:30,632 - INFO - Created 21 chunks for video_id 14\n",
      "2025-06-26 20:46:30,636 - INFO - Created 13 chunks for video_id 15\n",
      "2025-06-26 20:46:30,641 - INFO - Created 17 chunks for video_id 16\n",
      "2025-06-26 20:46:30,650 - INFO - Created 31 chunks for video_id 17\n",
      "2025-06-26 20:46:30,654 - INFO - Created 10 chunks for video_id 18\n",
      "2025-06-26 20:46:30,655 - INFO - Created 8 chunks for video_id 19\n",
      "2025-06-26 20:46:30,658 - INFO - Created 6 chunks for video_id 20\n",
      "2025-06-26 20:46:30,663 - INFO - Created 25 chunks for video_id 21\n",
      "2025-06-26 20:46:30,664 - INFO - Created 1 chunks for video_id 22\n",
      "2025-06-26 20:46:30,665 - INFO - Created 328 total LangChain documents\n",
      "2025-06-26 20:46:30,679 - INFO - Saved 328 transcript chunks to data/processed_transcripts.csv\n",
      "2025-06-26 20:46:30,687 - INFO - Validation report saved to docs/validation_report.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata success: 21/22\n",
      "Missing transcripts: 0/22\n",
      "Processed chunks: 328\n",
      "   video_id chunk_id                                               text  \\\n",
      "0         1      1_0  hey folk how you doing chris thanky here and i...   \n",
      "1         1      1_1  and issue and in those project youre going to ...   \n",
      "2         1      1_2  project the project status report any issue th...   \n",
      "3         1      1_3  be it would be crazy right it would take you f...   \n",
      "4         1      1_4  person and asking him this question and asking...   \n",
      "\n",
      "                                             subject  \n",
      "0  An AI Agent that knows everything about your P...  \n",
      "1  An AI Agent that knows everything about your P...  \n",
      "2  An AI Agent that knows everything about your P...  \n",
      "3  An AI Agent that knows everything about your P...  \n",
      "4  An AI Agent that knows everything about your P...  \n"
     ]
    }
   ],
   "source": [
    "def preprocess_transcripts():\n",
    "    df_clean = load_and_process_transcripts(transcript_file)\n",
    "    langchain_docs, missing_transcripts = prepare_langchain_docs(df_clean)\n",
    "    processed_data = []\n",
    "    for doc in langchain_docs:\n",
    "        processed_data.append({\n",
    "            'video_id': doc.metadata['video_id'],    \n",
    "            'chunk_id': doc.metadata['chunk_id'],\n",
    "            'text': doc.page_content,\n",
    "            'subject': doc.metadata['subject']\n",
    "        })\n",
    "    df_processed = pd.DataFrame(processed_data)\n",
    "    if df_processed.empty:\n",
    "        logging.error('No processed data generated')\n",
    "        raise ValueError('No processed data generated')\n",
    "    try:\n",
    "        df_processed.to_csv(output_file, index=False, encoding='utf-8')\n",
    "        logging.info(f'Saved {len(df_processed)} transcript chunks to {output_file}')\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error saving processed data: {e}')\n",
    "        raise\n",
    "    df_transcripts = pd.read_csv(transcript_file)\n",
    "    metadata_success = df_transcripts['title'].notnull().sum()\n",
    "    missing_trans = df_transcripts['transcript'].isna().sum()\n",
    "    total_chunks = len(df_processed)\n",
    "    sample_chunk = df_processed['text'].iloc[0] if not df_processed.empty else 'No chunks'\n",
    "    with open(validation_report, 'w') as f:\n",
    "        f.write(f'Transcripts: {len(df_transcripts)} videos, {missing_trans} missing\\n')\n",
    "        f.write(f'Processed Chunks: {total_chunks}\\n')\n",
    "        f.write(f'Sample Chunk:\\n{sample_chunk}\\n')\n",
    "        if missing_transcripts:\n",
    "            f.write(f'Missing transcripts for video IDs: {missing_transcripts}\\n')\n",
    "    logging.info(f'Validation report saved to {validation_report}')\n",
    "    print(f'Metadata success: {metadata_success}/{len(df_transcripts)}')\n",
    "    print(f'Missing transcripts: {missing_trans}/{len(df_transcripts)}')\n",
    "    print(f'Processed chunks: {total_chunks}')\n",
    "    if missing_transcripts:\n",
    "        print(f'Missing transcripts for video IDs: {missing_transcripts}')\n",
    "    print(df_processed.head())\n",
    "\n",
    "preprocess_transcripts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24af8586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 328\n",
      "Unique videos: 22\n",
      "   video_id chunk_id                                               text  \\\n",
      "0         1      1_0  hey folk how you doing chris thanky here and i...   \n",
      "1         1      1_1  and issue and in those project youre going to ...   \n",
      "2         1      1_2  project the project status report any issue th...   \n",
      "3         1      1_3  be it would be crazy right it would take you f...   \n",
      "4         1      1_4  person and asking him this question and asking...   \n",
      "\n",
      "                                             subject  \n",
      "0  An AI Agent that knows everything about your P...  \n",
      "1  An AI Agent that knows everything about your P...  \n",
      "2  An AI Agent that knows everything about your P...  \n",
      "3  An AI Agent that knows everything about your P...  \n",
      "4  An AI Agent that knows everything about your P...  \n"
     ]
    }
   ],
   "source": [
    "# Verify processed data\n",
    "try:\n",
    "    df_processed = pd.read_csv('data/processed_transcripts.csv')\n",
    "    print(f'Total chunks: {len(df_processed)}')\n",
    "    print(f'Unique videos: {df_processed[\"video_id\"].nunique()}')\n",
    "    print(df_processed[['video_id', 'chunk_id', 'text', 'subject']].head())\n",
    "except FileNotFoundError:\n",
    "    logging.error(f'Output file not found: {output_file}')\n",
    "    print(f'Output file not found: {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "345e1dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chunk_0</td>\n",
       "      <td>1</td>\n",
       "      <td>and to you where i plan to show you the greate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chunk_1</td>\n",
       "      <td>1</td>\n",
       "      <td>and issue and in those project youre going to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chunk_2</td>\n",
       "      <td>1</td>\n",
       "      <td>project the project status report any issue th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chunk_3</td>\n",
       "      <td>1</td>\n",
       "      <td>be it would be crazy right it would take you f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chunk_4</td>\n",
       "      <td>1</td>\n",
       "      <td>person and asking him this question and asking...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  chunk_id  video_id                                               text\n",
       "0  chunk_0         1  and to you where i plan to show you the greate...\n",
       "1  chunk_1         1  and issue and in those project youre going to ...\n",
       "2  chunk_2         1  project the project status report any issue th...\n",
       "3  chunk_3         1  be it would be crazy right it would take you f...\n",
       "4  chunk_4         1  person and asking him this question and asking..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Data/processed_transcripts.csv\")\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # normalize whitespace\n",
    "\n",
    "    # List of known filler phrases\n",
    "    filler_patterns = [\n",
    "        r\"\\b(hi|hey|hello|folks|how you doing|thank you|thanks|welcome|today|i am|i'm|this is .*?)\\b\",\n",
    "        r\"\\b(thanky here|and i\\b|let's talk about|so today|in this video)\\b\",\n",
    "        r\"\\b(folk|id like|to you|to this session|chris)\\b\",\n",
    "    ]\n",
    "\n",
    "    for pattern in filler_patterns:\n",
    "        text = re.sub(pattern, \"\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)  # clean double spaces\n",
    "    return text.strip()\n",
    "def chunk_transcript_by_sentences(text, max_sentences=4):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return [' '.join(sentences[i:i+max_sentences]) for i in range(0, len(sentences), max_sentences)]\n",
    "all_chunks = []\n",
    "\n",
    "for transcript in df[\"text\"]:  # Replace 'text' if your column is named differently\n",
    "    if pd.isna(transcript):\n",
    "        continue\n",
    "    cleaned = clean_text(transcript)\n",
    "    chunks = chunk_transcript_by_sentences(cleaned)\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "# Save cleaned chunks for use in RAG\n",
    "output_df = pd.DataFrame({\n",
    "    \"chunk_id\": [f\"chunk_{i}\" for i in range(len(all_chunks))],\n",
    "    \"video_id\": df[\"video_id\"].iloc[0] if not df.empty else \"unknown\",  # Assuming all chunks are from the same video\n",
    "    \"text\": all_chunks\n",
    "})\n",
    "\n",
    "output_df.to_csv(\"Data/processed_cleaned_chunks.csv\", index=False)\n",
    "output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f29a298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 20:49:09,086 - INFO - Use pytorch device_name: cpu\n",
      "2025-06-26 20:49:09,087 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "def store_in_pinecone(docs, index_name='youtube-transcripts', preview_file='chunk_previews.txt'):\n",
    "    load_dotenv()\n",
    "    pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "    if not pinecone_api_key:\n",
    "        logging.error('PINECONE_API_KEY not found in .env')\n",
    "        raise ValueError('PINECONE_API_KEY not found')\n",
    "\n",
    "    # Initialize Pinecone client\n",
    "    pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "    # Create index if not exists\n",
    "    if index_name not in [idx['name'] for idx in pc.list_indexes()]:\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=384,\n",
    "            metric='cosine',\n",
    "            spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
    "        )\n",
    "\n",
    "    embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    index = pc.Index(index_name)\n",
    "    vectors = []\n",
    "\n",
    "    with open(preview_file, 'w', encoding='utf-8') as f:\n",
    "        for i, doc in enumerate(docs):\n",
    "            embedding = embedder.encode(doc.page_content, show_progress_bar=False).tolist()\n",
    "            # Clean metadata: replace NaN/None with empty string\n",
    "            clean_metadata = {k: (\"\" if pd.isna(v) or v is None else v) for k, v in doc.metadata.items()}\n",
    "            clean_metadata['text'] = doc.page_content\n",
    "            vectors.append({\n",
    "                'id': doc.metadata['chunk_id'],\n",
    "                'values': embedding,\n",
    "                'metadata': clean_metadata\n",
    "            })\n",
    "\n",
    "            # Write a short preview to the file\n",
    "            preview_text = doc.page_content[:100].replace('\\n', ' ')  # limit to 100 chars, no new lines\n",
    "            f.write(f\"[{i+1}] Chunk ID: {doc.metadata['chunk_id']} | Video ID: {doc.metadata['video_id']} | Text: {preview_text}...\\n\")\n",
    "\n",
    "            if len(vectors) >= 100:\n",
    "                index.upsert(vectors=vectors)\n",
    "                vectors = []\n",
    "\n",
    "        if vectors:\n",
    "            index.upsert(vectors=vectors)\n",
    "\n",
    "    #logging.info(f'Upserted {len(docs)} vectors to Pinecone index {index_name}')\n",
    "    #logging.info(f'Chunk previews saved to {preview_file}')\n",
    "df = pd.read_csv(\"Data/processed_cleaned_chunks.csv\")\n",
    "\n",
    "# Convert cleaned chunks to LangChain Document format\n",
    "langchain_docs = [\n",
    "    Document(\n",
    "        page_content=row[\"text\"],\n",
    "        metadata={\n",
    "            \"chunk_id\": row[\"chunk_id\"],\n",
    "            \"video_id\": row[\"video_id\"]\n",
    "        }\n",
    "    )\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "store_in_pinecone(langchain_docs, index_name='youtube-transcripts', preview_file='docs/chunk_previews.txt') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85542b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(df, df_processed):\n",
    "    \"\"\"Validate input and processed data.\"\"\"\n",
    "    # Check for duplicates\n",
    "    duplicates = df[df.duplicated(subset=['video_id', 'youtube_link'], keep=False)]\n",
    "    if not duplicates.empty:\n",
    "        logging.warning(f'Found {len(duplicates)} duplicate video_id/youtube_link entries')\n",
    "    \n",
    "    # Validate YouTube links\n",
    "    invalid_links = df[~df['youtube_link'].str.contains(r'youtube\\.com|youtu\\.be', na=False)]\n",
    "    if not invalid_links.empty:\n",
    "        logging.warning(f'Found {len(invalid_links)} invalid YouTube links')\n",
    "    \n",
    "    # Chunk length stats\n",
    "    chunk_lengths = df_processed['text'].str.len()\n",
    "    chunk_stats = {\n",
    "        'avg_length': chunk_lengths.mean(),\n",
    "        'min_length': chunk_lengths.min(),\n",
    "        'max_length': chunk_lengths.max()\n",
    "    }\n",
    "    logging.info(f'Chunk length stats: {chunk_stats}')\n",
    "    return chunk_stats\n",
    "\n",
    "# In preprocess_transcripts(), before saving:\n",
    "chunk_stats = validate_data(df_clean, df_processed)\n",
    "with open(validation_report, 'a') as f:\n",
    "    f.write(f'Chunk Length Stats: Average={chunk_stats[\"avg_length\"]:.1f}, Min={chunk_stats[\"min_length\"]}, Max={chunk_stats[\"max_length\"]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d698da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update project documentation\n",
    "with open('docs/project_log.md', 'a') as f:\n",
    "    f.write('## Data Collection and Preprocessing\\n')\n",
    "    f.write('- Loaded YouTube video from `data/SNOW_YT_Videos.csv`.\\n')\n",
    "    f.write('- Loaded YouTube video metadata to `data/ServiceNow_Youtube_Metadata_Clean.csv`.\\n')\n",
    "    f.write('- Loaded YouTube video transcripts to `data/video_metadata_with_transcripts.csv`.\\n')\n",
    "    f.write('- Preprocessed transcripts with NLTK lemmatization and LangChain text splitting (chunk_size=500, overlap=50).\\n')\n",
    "    f.write(f'- Processed 22 videos, generating 328 chunks.\\n')\n",
    "    f.write('- Saved processed data to `data/processed_transcripts.csv`.\\n')\n",
    "    f.write('- Challenges: Resolved KeyError by standardizing column names (e.g., Number to video_id).\\n')\n",
    "    f.write('- Validation report saved to `docs/validation_report.txt`.\\n')\n",
    "    f.write('- Chunk Preview Data saved to `docs/chunk_preview.csv`.\\n')\n",
    "    f.write(f'  Average: {chunk_stats[\"avg_length\"]:.1f}\\n')\n",
    "    f.write(f'  Minimum: {chunk_stats[\"min_length\"]}\\n')\n",
    "    f.write(f'  Maximum: {chunk_stats[\"max_length\"]}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
