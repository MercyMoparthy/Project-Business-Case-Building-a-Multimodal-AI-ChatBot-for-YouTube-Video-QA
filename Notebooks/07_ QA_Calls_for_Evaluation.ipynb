{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a4a61aa",
   "metadata": {},
   "source": [
    "## Model:QA Calls For Evaluation\n",
    "Logging every call to the chain for later review/analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53bbdc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\python.exe -m pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\python.exe -m pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\python.exe -m pip install [options] [-e] <vcs project url> ...\n",
      "  c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\python.exe -m pip install [options] [-e] <local project path> ...\n",
      "  c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\python.exe -m pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: -y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files removed: 166 (92.2 MB)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain langsmith -y\n",
    "%pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4c48c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optimum\n",
      "  Downloading optimum-1.26.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: transformers>=4.29 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from optimum) (4.53.0)\n",
      "Requirement already satisfied: torch>=1.11 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from optimum) (2.7.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from optimum) (24.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from optimum) (2.2.6)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from optimum) (0.33.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from huggingface-hub>=0.8.0->optimum) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from huggingface-hub>=0.8.0->optimum) (2025.5.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from huggingface-hub>=0.8.0->optimum) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from huggingface-hub>=0.8.0->optimum) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from huggingface-hub>=0.8.0->optimum) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from huggingface-hub>=0.8.0->optimum) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from torch>=1.11->optimum) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from torch>=1.11->optimum) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from torch>=1.11->optimum) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11->optimum) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.8.0->optimum) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from transformers>=4.29->optimum) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from transformers>=4.29->optimum) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from transformers>=4.29->optimum) (0.5.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from jinja2->torch>=1.11->optimum) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2025.6.15)\n",
      "Downloading optimum-1.26.1-py3-none-any.whl (424 kB)\n",
      "Installing collected packages: optimum\n",
      "Successfully installed optimum-1.26.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14271fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import pipeline\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4302eea0",
   "metadata": {},
   "source": [
    "## Set Up Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "535f7e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_test_set = [\n",
    "    {\"question\": \"What is ServiceNow?\", \"gold_answer\": \"ServiceNow is a cloud platform.\", \"audio_file\": \"audio1.wav\", \"gold_transcript\": \"ServiceNow is a cloud platform.\"},\n",
    "    ...\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a1b44c",
   "metadata": {},
   "source": [
    " ## Define Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3872408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not load model TheBloke/Mistral-7B-Instruct-v0.2-GPTQ with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 292, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 600, in from_pretrained\n    return model_class.from_pretrained(\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\modeling_utils.py\", line 311, in _wrapper\n    return func(*args, **kwargs)\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\modeling_utils.py\", line 4642, in from_pretrained\n    hf_quantizer.validate_environment(\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\quantizers\\quantizer_gptq.py\", line 65, in validate_environment\n    raise RuntimeError(\"GPU is required to quantize or run quantize model.\")\nRuntimeError: GPU is required to quantize or run quantize model.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 310, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 600, in from_pretrained\n    return model_class.from_pretrained(\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\modeling_utils.py\", line 311, in _wrapper\n    return func(*args, **kwargs)\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\modeling_utils.py\", line 4642, in from_pretrained\n    hf_quantizer.validate_environment(\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\quantizers\\quantizer_gptq.py\", line 65, in validate_environment\n    raise RuntimeError(\"GPU is required to quantize or run quantize model.\")\nRuntimeError: GPU is required to quantize or run quantize model.\n\nwhile loading with MistralForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 292, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\modeling_utils.py\", line 311, in _wrapper\n    return func(*args, **kwargs)\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\modeling_utils.py\", line 4642, in from_pretrained\n    hf_quantizer.validate_environment(\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\quantizers\\quantizer_gptq.py\", line 65, in validate_environment\n    raise RuntimeError(\"GPU is required to quantize or run quantize model.\")\nRuntimeError: GPU is required to quantize or run quantize model.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 310, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\modeling_utils.py\", line 311, in _wrapper\n    return func(*args, **kwargs)\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\modeling_utils.py\", line 4642, in from_pretrained\n    hf_quantizer.validate_environment(\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\quantizers\\quantizer_gptq.py\", line 65, in validate_environment\n    raise RuntimeError(\"GPU is required to quantize or run quantize model.\")\nRuntimeError: GPU is required to quantize or run quantize model.\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m      3\u001b[0m hf_pipes \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m: pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflan-t5-base\u001b[39m\u001b[38;5;124m\"\u001b[39m: pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext2text-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/flan-t5-base\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflan-t5-large\u001b[39m\u001b[38;5;124m\"\u001b[39m: pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext2text-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/flan-t5-large\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral-7b\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTheBloke/Mistral-7B-Instruct-v0.2-GPTQ\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[0;32m      8\u001b[0m }\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 2. Define OpenAI function\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mask_gpt3_turbo\u001b[39m(prompt):\n",
      "File \u001b[1;32mc:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\pipelines\\__init__.py:1030\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1029\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m-> 1030\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m   1031\u001b[0m         adapter_path \u001b[38;5;28;01mif\u001b[39;00m adapter_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m model,\n\u001b[0;32m   1032\u001b[0m         model_classes\u001b[38;5;241m=\u001b[39mmodel_classes,\n\u001b[0;32m   1033\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m   1034\u001b[0m         framework\u001b[38;5;241m=\u001b[39mframework,\n\u001b[0;32m   1035\u001b[0m         task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[0;32m   1036\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m   1037\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1038\u001b[0m     )\n\u001b[0;32m   1040\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m   1041\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32mc:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\pipelines\\base.py:332\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    331\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 332\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    333\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    334\u001b[0m         )\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    337\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Could not load model TheBloke/Mistral-7B-Instruct-v0.2-GPTQ with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 292, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 600, in from_pretrained\n    return model_class.from_pretrained(\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\modeling_utils.py\", line 311, in _wrapper\n    return func(*args, **kwargs)\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\modeling_utils.py\", line 4642, in from_pretrained\n    hf_quantizer.validate_environment(\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\quantizers\\quantizer_gptq.py\", line 65, in validate_environment\n    raise RuntimeError(\"GPU is required to quantize or run quantize model.\")\nRuntimeError: GPU is required to quantize or run quantize model.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 310, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 600, in from_pretrained\n    return model_class.from_pretrained(\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\modeling_utils.py\", line 311, in _wrapper\n    return func(*args, **kwargs)\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\modeling_utils.py\", line 4642, in from_pretrained\n    hf_quantizer.validate_environment(\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\quantizers\\quantizer_gptq.py\", line 65, in validate_environment\n    raise RuntimeError(\"GPU is required to quantize or run quantize model.\")\nRuntimeError: GPU is required to quantize or run quantize model.\n\nwhile loading with MistralForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 292, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\modeling_utils.py\", line 311, in _wrapper\n    return func(*args, **kwargs)\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\modeling_utils.py\", line 4642, in from_pretrained\n    hf_quantizer.validate_environment(\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\quantizers\\quantizer_gptq.py\", line 65, in validate_environment\n    raise RuntimeError(\"GPU is required to quantize or run quantize model.\")\nRuntimeError: GPU is required to quantize or run quantize model.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 310, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\modeling_utils.py\", line 311, in _wrapper\n    return func(*args, **kwargs)\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\modeling_utils.py\", line 4642, in from_pretrained\n    hf_quantizer.validate_environment(\n  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\transformers\\quantizers\\quantizer_gptq.py\", line 65, in validate_environment\n    raise RuntimeError(\"GPU is required to quantize or run quantize model.\")\nRuntimeError: GPU is required to quantize or run quantize model.\n\n\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "hf_pipes = {\n",
    "    \"distilgpt2\": pipeline(\"text-generation\", model=\"distilgpt2\"),\n",
    "    \"flan-t5-base\": pipeline(\"text2text-generation\", model=\"google/flan-t5-base\"),\n",
    "    \"flan-t5-large\": pipeline(\"text2text-generation\", model=\"google/flan-t5-large\"),\n",
    "    \"mistral-7b\": pipeline(\"text-generation\", model=\"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"),\n",
    "}\n",
    "\n",
    "# 2. Define OpenAI function\n",
    "def ask_gpt3_turbo(prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\", messages=[{\"role\":\"user\",\"content\":prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b371f378",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_asr = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef8119f",
   "metadata": {},
   "source": [
    "## Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42792052",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# Use audio_test_set as test_set if it contains the required fields\n",
    "test_set = audio_test_set\n",
    "\n",
    "for example in test_set:  # test_set: list of dicts\n",
    "    q = example['question']\n",
    "    gold = example['gold_answer']\n",
    "    row = {'question': q, 'gold_answer': gold}\n",
    "\n",
    "    # Hugging Face models\n",
    "    for name, pipe in hf_pipes.items():\n",
    "        if 't5' in name:  # text2text\n",
    "            out = pipe(q)[0]['generated_text']\n",
    "        else:  # text-generation\n",
    "            out = pipe(q, max_new_tokens=64)[0]['generated_text']\n",
    "        row[name] = out\n",
    "\n",
    "    # OpenAI\n",
    "    row[\"gpt-3.5-turbo\"] = ask_gpt3_turbo(q)\n",
    "    results.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a9f921",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in audio_test_set:\n",
    "    audio_path = example['audio_file']\n",
    "    gold = example['gold_transcript']\n",
    "    asr_out = whisper_asr(audio_path)['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0349d0e0",
   "metadata": {},
   "source": [
    "## Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5480e130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def token_f1(a, b):\n",
    "    a, b = set(a.lower().split()), set(b.lower().split())\n",
    "    if not a or not b: return 0\n",
    "    common = a & b\n",
    "    precision = len(common)/len(a)\n",
    "    recall = len(common)/len(b)\n",
    "    return 2*precision*recall/(precision+recall+1e-8)\n",
    "\n",
    "for r in results:\n",
    "    for model in hf_pipes.keys() + [\"gpt-3.5-turbo\"]:\n",
    "        r[f'{model}_f1'] = token_f1(r[model], r['gold_answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b45e628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiwer  # pip install jiwer\n",
    "for example in audio_test_set:\n",
    "    asr_out = whisper_asr(example['audio_file'])['text']\n",
    "    example['wer'] = jiwer.wer(example['gold_transcript'], asr_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6c2558",
   "metadata": {},
   "source": [
    "## Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e4d3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "print(df[[f'{model}_f1' for model in hf_pipes.keys() + [\"gpt-3.5-turbo\"]]].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f75608",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df = pd.DataFrame(audio_test_set)\n",
    "print(asr_df['wer'].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
