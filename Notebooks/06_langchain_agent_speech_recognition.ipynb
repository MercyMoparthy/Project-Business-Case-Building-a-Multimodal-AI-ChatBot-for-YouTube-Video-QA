{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a5b681f",
   "metadata": {},
   "source": [
    "## LangChain Agent + Speech Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20302209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "from langchain.tools import Tool\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8664ad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model = whisper.load_model(\"base\")\n",
    "def transcribe_audio(file_path):\n",
    "    print(\"DEBUG: file_path =\", file_path, type(file_path))\n",
    "    result = whisper_model.transcribe(file_path)\n",
    "    return result[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "557077c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_qa(query):\n",
    "    return rag_pipeline(query)\n",
    "qa_tool = Tool(\n",
    "    name=\"YouTubeQA\",\n",
    "    func=rag_qa,\n",
    "    description=\"Answer questions about YouTube videos using RAG.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d107c4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mercy\\AppData\\Local\\Temp\\ipykernel_17520\\3760734133.py:1: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
      "C:\\Users\\Mercy\\AppData\\Local\\Temp\\ipykernel_17520\\3760734133.py:3: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "tools = [qa_tool]\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db2ef56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: file_path = C:\\Users\\Mercy\\AppData\\Local\\Temp\\gradio\\aefc7cc471dab426274c54cc64f601b27774d5cbe36df096c8d6c6d695c06f3b\\audio.wav <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "C:\\Users\\Mercy\\AppData\\Local\\Temp\\ipykernel_17520\\2344364971.py:3: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  answer = agent.run(input=text_query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mServiceNow is a cloud-based platform that provides IT service management (ITSM), as well as digital workflow automation. It helps organizations manage their IT services, automate routine tasks, and streamline business processes. ServiceNow offers a wide range of tools and applications to help businesses improve efficiency, productivity, and customer satisfaction.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 403, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\applications.py\", line 112, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 187, in __call__\n",
      "    raise exc\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 165, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\gradio\\route_utils.py\", line 850, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\routing.py\", line 714, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\routing.py\", line 734, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\routing.py\", line 288, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\routing.py\", line 76, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\routing.py\", line 74, in app\n",
      "    await response(scope, receive, send)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\responses.py\", line 359, in __call__\n",
      "    await self._handle_simple(send, send_header_only)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\responses.py\", line 388, in _handle_simple\n",
      "    await send({\"type\": \"http.response.body\", \"body\": chunk, \"more_body\": more_body})\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\_exception_handler.py\", line 39, in sender\n",
      "    await send(message)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\_exception_handler.py\", line 39, in sender\n",
      "    await send(message)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 162, in _send\n",
      "    await send(message)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 507, in send\n",
      "    output = self.conn.send(event=h11.EndOfMessage())\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\h11\\_connection.py\", line 538, in send\n",
      "    data_list = self.send_with_data_passthrough(event)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\h11\\_connection.py\", line 571, in send_with_data_passthrough\n",
      "    writer(event, data_list.append)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\h11\\_writers.py\", line 67, in __call__\n",
      "    self.send_eom(event.headers, write)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\h11\\_writers.py\", line 96, in send_eom\n",
      "    raise LocalProtocolError(\"Too little data for declared Content-Length\")\n",
      "h11._util.LocalProtocolError: Too little data for declared Content-Length\n",
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 403, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\applications.py\", line 112, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 187, in __call__\n",
      "    raise exc\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 165, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\gradio\\route_utils.py\", line 850, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\routing.py\", line 714, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\routing.py\", line 734, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\routing.py\", line 288, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\routing.py\", line 76, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\routing.py\", line 74, in app\n",
      "    await response(scope, receive, send)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\responses.py\", line 359, in __call__\n",
      "    await self._handle_simple(send, send_header_only)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\responses.py\", line 388, in _handle_simple\n",
      "    await send({\"type\": \"http.response.body\", \"body\": chunk, \"more_body\": more_body})\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\_exception_handler.py\", line 39, in sender\n",
      "    await send(message)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\_exception_handler.py\", line 39, in sender\n",
      "    await send(message)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 162, in _send\n",
      "    await send(message)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 507, in send\n",
      "    output = self.conn.send(event=h11.EndOfMessage())\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\h11\\_connection.py\", line 538, in send\n",
      "    data_list = self.send_with_data_passthrough(event)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\h11\\_connection.py\", line 571, in send_with_data_passthrough\n",
      "    writer(event, data_list.append)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\h11\\_writers.py\", line 67, in __call__\n",
      "    self.send_eom(event.headers, write)\n",
      "  File \"c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\h11\\_writers.py\", line 96, in send_eom\n",
      "    raise LocalProtocolError(\"Too little data for declared Content-Length\")\n",
      "h11._util.LocalProtocolError: Too little data for declared Content-Length\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: file_path = C:\\Users\\Mercy\\AppData\\Local\\Temp\\gradio\\5ce6baffdd050984bdc27d17b4960e91fdd51999d4709b0a7cd4204db2c97338\\audio.wav <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mercy\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mIn ServiceNow, AI functionalities are used to automate and streamline various processes within the platform. Some common AI functionalities in ServiceNow include:\n",
      "\n",
      "1. Virtual Agents: AI-powered chatbots that can assist users with common IT service requests, such as password resets, incident reporting, and knowledge base searches.\n",
      "\n",
      "2. Predictive Intelligence: AI algorithms that analyze historical data to predict potential issues, such as outages or equipment failures, before they occur.\n",
      "\n",
      "3. Natural Language Understanding: AI technology that enables users to interact with the ServiceNow platform using natural language queries, making it easier to search for information or create tickets.\n",
      "\n",
      "4. Machine Learning: AI algorithms that can learn from data and make predictions or recommendations based on patterns and trends, such as identifying recurring issues or anomalies in IT operations.\n",
      "\n",
      "5. Intelligent Automation: AI-driven automation capabilities that can automate repetitive tasks, such as assigning tickets, categorizing incidents, or routing requests to the appropriate teams.\n",
      "\n",
      "Overall, AI functionalities in ServiceNow help organizations improve efficiency, enhance user experience, and optimize IT service management processes.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def process_audio_with_agent(audio_file):\n",
    "    text_query = transcribe_audio(audio_file)\n",
    "    answer = agent.run(input=text_query)\n",
    "    return f\"Q: {text_query}\\n\\nA: {answer}\"\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=process_audio_with_agent,\n",
    "    inputs=gr.Audio(type=\"filepath\", label=\"Record or Upload Audio\"),\n",
    "    outputs=gr.Textbox(label=\"Q & A\", lines=6),\n",
    "    title=\"ServiceNow QA Assistant: Ask by Voice!\",\n",
    "    description=\"The answer will appear below your transcribed question.\"\n",
    ")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c68c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mercy\\AppData\\Local\\Temp\\ipykernel_17520\\3495208296.py:18: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label=\"Conversation\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7873\n",
      "* Running on public URL: https://cbd9e59567dff68429.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://cbd9e59567dff68429.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mServiceNow is a cloud-based platform that provides IT service management (ITSM), as well as other service management solutions for various departments within an organization. It helps organizations streamline their operations, automate workflows, and improve efficiency by providing tools for service desk management, incident management, change management, and more. ServiceNow is widely used by businesses to optimize their service delivery and enhance customer satisfaction.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def process_query(text_query, audio_file, history):\n",
    "    if audio_file is not None:\n",
    "        text_query = transcribe_audio(audio_file)\n",
    "        question = text_query\n",
    "    elif text_query and text_query.strip():\n",
    "        question = text_query.strip()\n",
    "    else:\n",
    "        return history + [(\"User\", \"Please enter a question or upload audio.\")]\n",
    "    answer = agent.run(input=question)\n",
    "    history = history + [(f\"User: {question}\", f\"Agent: {answer}\")]\n",
    "    return history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.HTML(\"<h1 style='text-align: center;'>ServiceNow QA Agent</h1>\")\n",
    "    gr.Markdown(\"<center>Type or record your question below. The bot will provide you answer</center>\")\n",
    "    chatbot = gr.Chatbot(label=\"Conversation\")\n",
    "    with gr.Row():\n",
    "        text_input = gr.Textbox(label=\"Type Your Question\", lines=2)\n",
    "        audio_input = gr.Audio(type=\"filepath\", label=\"Or Record/Upload Audio\")\n",
    "    submit = gr.Button(\"Submit\")\n",
    "    #clear = gr.Button(\"Clear Chat\")\n",
    "\n",
    "    state = gr.State([])  # to hold chat history\n",
    "\n",
    "    submit.click(\n",
    "        process_query,\n",
    "        inputs=[text_input, audio_input, state],\n",
    "        outputs=chatbot\n",
    "    ).then(\n",
    "        lambda history: history,  # update state with latest history\n",
    "        inputs=chatbot,\n",
    "        outputs=state\n",
    "    )\n",
    "    #clear.click(\n",
    "        #lambda: [],\n",
    "        #None,\n",
    "        #[chatbot, state]\n",
    "    #)\n",
    "\n",
    "demo.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
