{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Tl5t2-ViO2YV"
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain langchain-community langchain-openai sentence-transformers faiss-cpu gradio --quiet\n",
        "!pip install openai-whisper --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LzwwQwQHRIt8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_openai import OpenAI\n",
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_openai import ChatOpenAI\n",
        "from pathlib import Path\n",
        "from langchain.docstore.document import Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H45TkcP9CRAZ",
        "outputId": "ac9ffaaa-b174-4a88-effe-822059faecee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "63471d69d3074ab3bc7fe17858527a87",
            "83a7fb19f9a840cb999a11aa29a660ab",
            "a5d0db4ef85d415bafd7f141f992b964",
            "f8ff5599aca9402e93231b4b201aec56",
            "5159975a768d4698a9f315f24901fa9b",
            "fc9ec0642b974aca9a17f8a292844aa6",
            "b391aa9502e244cba111b56543665617",
            "23d55aad06f2423bbaa71e6e338dd3f5",
            "b2381747d1a14e7cac15623809c96061",
            "1073e3fd4ea74f28b019958f7fc36de8",
            "00da92926f3a49fe85d66b47f86e88e0"
          ]
        },
        "id": "xLmJbIPRRNuo",
        "outputId": "93b869bf-eef8-46d1-bc85-facf80d61d81"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Chunking transcripts: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "63471d69d3074ab3bc7fe17858527a87"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded and chunked 656 document chunks from /content/drive/MyDrive/ServiceNow_Audio_Transcripts.\n"
          ]
        }
      ],
      "source": [
        "transcript_folder = Path(\"/content/drive/MyDrive/ServiceNow_Audio_Transcripts\")\n",
        "transcript_folder.mkdir(parents=True, exist_ok=True)\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "\n",
        "documents: list[Document] = []\n",
        "for txt_path in tqdm(transcript_folder.glob(\"*.txt\"), desc=\"Chunking transcripts\"):\n",
        "    raw_text = txt_path.read_text(encoding=\"utf-8\")\n",
        "    lines = raw_text.splitlines()\n",
        "    subject = lines[0] if lines else txt_path.stem  # Use first line or fallback to filename\n",
        "    for chunk in splitter.split_text(raw_text):\n",
        "        documents.append(\n",
        "            Document(page_content=chunk, metadata={\"source\": subject})\n",
        "        )\n",
        "print(f\"✅ Loaded and chunked {len(documents)} document chunks from {transcript_folder}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWl-S-b2RPuJ",
        "outputId": "b7a2e38a-d522-4d92-97f4-4a05302631f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-5-1688787792.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
          ]
        }
      ],
      "source": [
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "faiss_dir = \"/content/drive/MyDrive/faiss_store\"\n",
        "faiss_dir = Path(faiss_dir)\n",
        "Path(faiss_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "faiss_index = FAISS.load_local(\n",
        "    faiss_dir,\n",
        "    embedding_model,\n",
        "    allow_dangerous_deserialization=True\n",
        ")\n",
        "retriever = faiss_index.as_retriever(search_kwargs={\"k\": 5})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KfvjXdnkRRbL"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(\n",
        "    temperature=0,\n",
        "    openai_api_key=userdata.get(\"OPENAI_API_KEY\")\n",
        ")\n",
        "\n",
        "retriever = faiss_index.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 5}\n",
        ")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRsN5x83PJNQ",
        "outputId": "7e757393-d964-4ec7-9fa7-81c129605d80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "from typing import Tuple, Union\n",
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "\n",
        "transcriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-small\")\n",
        "\n",
        "def answer_question(text_input: str, audio_input: Union[str, dict, None]) -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Runs the RetrievalQA chain and formats the answer along with source snippets.\n",
        "    Handles both text and audio input.\n",
        "\n",
        "    Args:\n",
        "        text_input: The string input from the text box.\n",
        "        audio_input: The audio input, expected as a string filepath from Gradio.\n",
        "\n",
        "    Returns:\n",
        "      - answer text\n",
        "      - formatted source list (one entry per document)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        question = \"\"\n",
        "        print(f\"Debug: text_input type: {type(text_input)}, value: '{text_input}'\")\n",
        "        print(f\"Debug: audio_input type: {type(audio_input)}, value: {audio_input}\")\n",
        "\n",
        "        if isinstance(audio_input, str) and audio_input:\n",
        "             audio_filepath = audio_input\n",
        "             print(f\"Debug: Processing audio from filepath: {audio_filepath}\")\n",
        "             try:\n",
        "                 transcription_result = transcriber(audio_filepath)\n",
        "                 print(f\"Debug: Transcription result: {transcription_result}\")\n",
        "                 question = transcription_result.get(\"text\", \"\")\n",
        "                 if not question:\n",
        "                     print(\"Debug: Transcription resulted in empty string.\")\n",
        "                     return \"Could not transcribe audio. Please try again.\", \"\"\n",
        "                 question = question.strip()\n",
        "                 if not question:\n",
        "                     print(\"Debug: Stripped transcription is empty.\")\n",
        "                     return \"Transcribed audio was empty. Please try again.\", \"\"\n",
        "             except Exception as trans_e:\n",
        "                 print(f\"Debug: Error during transcription: {trans_e}\")\n",
        "                 return f\"Error during transcription: {trans_e}\", \"\"\n",
        "\n",
        "        if not question and isinstance(text_input, str) and text_input.strip():\n",
        "            print(\"Debug: Using text input.\")\n",
        "            question = text_input.strip()\n",
        "            if not question:\n",
        "                print(\"Debug: Stripped text input is empty.\")\n",
        "                return \"Please enter a question or record audio.\", \"\"\n",
        "\n",
        "        if not question:\n",
        "             print(\"Debug: Final question is empty after checking both inputs.\")\n",
        "             return \"No valid question provided via text or audio.\", \"\"\n",
        "\n",
        "        print(f\"Debug: Final question for QA chain: '{question}'\")\n",
        "\n",
        "        try:\n",
        "            print(\"Debug: Calling qa_chain...\")\n",
        "            response = qa_chain(question)\n",
        "            print(f\"Debug: qa_chain response type: {type(response)}\")\n",
        "            print(f\"Debug: qa_chain response keys: {response.keys() if isinstance(response, dict) else 'Not a dict'}\")\n",
        "            answer = response.get(\"result\", \"\")\n",
        "            print(f\"Debug: Answer extracted: '{answer}')\")\n",
        "\n",
        "            sources = []\n",
        "            source_documents = response.get(\"source_documents\", [])\n",
        "            print(f\"Debug: Number of source documents: {len(source_documents)}\")\n",
        "            for doc in source_documents:\n",
        "                text = doc.page_content.strip()\n",
        "                preview = text[:400] + \"...\" if len(text) > 400 else text\n",
        "                sources.append(f\"📄 {doc.metadata.get('source', 'unknown')}\\n🔎 {preview}\")\n",
        "            print(f\"Debug: Sources formatted. Number of formatted sources: {len(sources)}\")\n",
        "\n",
        "            if not answer:\n",
        "                 print(\"Debug: Answer is empty after qa_chain.\")\n",
        "                 return \"Could not find a relevant answer in the documents.\", \"\\n\\n\".join(sources)\n",
        "\n",
        "\n",
        "            return answer, \"\\n\\n\".join(sources)\n",
        "\n",
        "        except Exception as qa_e:\n",
        "            print(f\"Debug: Error during qa_chain execution or response processing: {qa_e}\")\n",
        "            return f\"Error processing question with QA chain: {qa_e}\", \"\"\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        err = f\"An unexpected error occurred in answer_question: {e}\"\n",
        "        print(f\"Debug: Unexpected error in answer_question: {e}\")\n",
        "        return err, err"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "from typing import Union, List, Dict\n",
        "\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "# ✅ Load FAISS and LLM\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "faiss_dir = \"/content/drive/MyDrive/faiss_store\"\n",
        "faiss_dir = Path(faiss_dir)\n",
        "Path(faiss_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "faiss_index = FAISS.load_local(\n",
        "    faiss_dir,\n",
        "    embedding_model,\n",
        "    allow_dangerous_deserialization=True\n",
        ")\n",
        "\n",
        "retriever = faiss_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
        "\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "if not openai_api_key:\n",
        "    raise ValueError(\"❌ Set your OPENAI_API_KEY in environment or secrets.\")\n",
        "\n",
        "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "\n",
        "# ✅ This is your actual QA chain — required for Gradio to work!\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)\n",
        "\n",
        "# ✅ Whisper transcription\n",
        "transcriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-small\")\n",
        "\n",
        "# ✅ Gradio function\n",
        "def answer_question(text_input: str, audio_input: Union[str, dict, None], chat_history: List[Dict]) -> List[Dict]:\n",
        "    question = \"\"\n",
        "    if isinstance(audio_input, str) and audio_input:\n",
        "        result = transcriber(audio_input)\n",
        "        question = result.get(\"text\", \"\").strip()\n",
        "    elif text_input:\n",
        "        question = text_input.strip()\n",
        "\n",
        "    if not question:\n",
        "        return chat_history + [{\"role\": \"assistant\", \"content\": \"❌ Please ask a question via text or audio.\"}]\n",
        "\n",
        "    try:\n",
        "        response = qa_chain(question)\n",
        "        answer = response.get(\"result\", \"❌ No answer found.\")\n",
        "        sources = []\n",
        "\n",
        "        for doc in response.get(\"source_documents\", []):\n",
        "            preview = doc.page_content[:200]\n",
        "            source = doc.metadata.get(\"source\", \"unknown\")\n",
        "            sources.append(f\"📄 {source}\\n🔎 {preview}\")\n",
        "\n",
        "        full_answer = answer + \"\\n\\n\"\n",
        "        chat_history.append({\"role\": \"user\", \"content\": question})\n",
        "        chat_history.append({\"role\": \"assistant\", \"content\": full_answer})\n",
        "        return chat_history\n",
        "\n",
        "    except Exception as e:\n",
        "        chat_history.append({\"role\": \"assistant\", \"content\": f\"❌ Error from QA chain: {e}\"})\n",
        "        return chat_history\n",
        "\n",
        "# ✅ Gradio UI\n",
        "with gr.Blocks() as demo:\n",
        "    gr.HTML(\"<h1 style='text-align: center;'>ServiceNow QA Assistant</h1>\")\n",
        "    gr.Markdown(\"<center>Type or record your question below. The bot will provide you answer</center>\")\n",
        "    chatbot = gr.Chatbot(label=\"💬 ServiceNow Assistant\", type=\"messages\", value=[\n",
        "        {\"role\": \"assistant\", \"content\": \"👋 Hi! Ask me anything about ServiceNow (text or voice).\"}\n",
        "    ])\n",
        "    with gr.Row():\n",
        "        text_input = gr.Textbox(placeholder=\"Type your ServiceNow question here...\", label=\"📝 Text\")\n",
        "        audio_input = gr.Audio(type=\"filepath\", label=\"🎙️ Upload Your Voice\")\n",
        "    submit_btn = gr.Button(\"🔍 Ask\")\n",
        "\n",
        "    submit_btn.click(fn=answer_question, inputs=[text_input, audio_input, chatbot], outputs=chatbot)\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "UbY5QDCjn3Qc",
        "outputId": "527a43f7-21e5-4da9-8a06-31ad79492113",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://5071bbae932a5f3001.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://5071bbae932a5f3001.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "code = '''import os\n",
        "from typing import Tuple, Union\n",
        "import gradio as gr\n",
        "from langchain_openai import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from pathlib import Path\n",
        "from transformers import pipeline\n",
        "from google.colab import userdata # Assuming you'll run this part in Colab to get the key\n",
        "\n",
        "# --- Configuration ---\n",
        "# You might need to securely load this in a production app (e.g., using environment variables)\n",
        "# For Colab, you can get it from userdata secrets\n",
        "openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "if not openai_api_key:\n",
        "    raise EnvironmentError(\"OPENAI_API_KEY not set. Please add it to Colab secrets.\")\n",
        "\n",
        "# Define the path to your saved FAISS index\n",
        "faiss_store = \"/content/drive/MyDrive/faiss_store\"\n",
        "if not Path(faiss_store).exists():\n",
        "     raise FileNotFoundError(f\"FAISS index not found at {faiss_store}. Please ensure it's saved there.\")\n",
        "\n",
        "# --- Model and Chain Initialization ---\n",
        "\n",
        "# Load the speech-to-text model\n",
        "transcriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-small\")\n",
        "\n",
        "# Initialize the embedding model (used for loading FAISS)\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Load the FAISS index from the local directory\n",
        "faiss_index = FAISS.load_local(\n",
        "    faiss_store,\n",
        "    embedding_model,\n",
        "    allow_dangerous_deserialization=True # Be cautious with untrusted sources\n",
        ")\n",
        "\n",
        "# Initialize the OpenAI LLM\n",
        "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "\n",
        "# Create the RetrievalQA chain\n",
        "retriever = faiss_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "# --- Gradio Interface Function ---\n",
        "\n",
        "def answer_question(text_input: str, audio_input: Union[str, dict, None]) -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Runs the RetrievalQA chain and formats the answer along with source snippets.\n",
        "    Handles both text and audio input.\n",
        "\n",
        "    Args:\n",
        "        text_input: The string input from the text box.\n",
        "        audio_input: The audio input, expected as a string filepath from Gradio.\n",
        "\n",
        "    Returns:\n",
        "      - answer text\n",
        "      - formatted source list (one entry per document)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        question = \"\"\n",
        "\n",
        "        # Process audio input if provided (handle as a string filepath)\n",
        "        if isinstance(audio_input, str) and audio_input:\n",
        "             audio_filepath = audio_input\n",
        "             try:\n",
        "                 transcription_result = transcriber(audio_filepath)\n",
        "                 question = transcription_result.get(\"text\", \"\")\n",
        "                 if not question:\n",
        "                     return \"Could not transcribe audio. Please try again.\", \"\"\n",
        "                 question = question.strip() # Remove leading/trailing whitespace from transcription\n",
        "                 if not question:\n",
        "                     return \"Transcribed audio was empty. Please try again.\", \"\"\n",
        "             except Exception as trans_e:\n",
        "                 return f\"Error during transcription: {trans_e}\", \"\"\n",
        "\n",
        "\n",
        "        # Use text input if audio was not provided or transcription failed\n",
        "        if not question and isinstance(text_input, str) and text_input.strip():\n",
        "            question = text_input.strip()\n",
        "            if not question:\n",
        "                return \"Please enter a question or record audio.\", \"\"\n",
        "\n",
        "        # Ensure there is a question to process after handling input types\n",
        "        if not question:\n",
        "             return \"No valid question provided via text or audio.\", \"\"\n",
        "\n",
        "        # Run the QA chain with the processed question\n",
        "        try:\n",
        "            response = qa_chain(question)\n",
        "            answer = response.get(\"result\", \"\") # Use .get() with a default value\n",
        "\n",
        "            # Build source list with preview snippets\n",
        "            sources = []\n",
        "            source_documents = response.get(\"source_documents\", []) # Use .get() with a default empty list\n",
        "            for doc in source_documents:\n",
        "                text = doc.page_content.strip()\n",
        "                preview = text[:400] + \"...\" if len(text) > 400 else text\n",
        "                sources.append(f\"📄 {doc.metadata.get('source', 'unknown')}\\n🔎 {preview}\")\n",
        "\n",
        "            if not answer:\n",
        "                 # It's possible the QA chain returned successfully but with no relevant answer\n",
        "                 return \"Could not find a relevant answer in the documents.\", \"\\n\\n\".join(sources)\n",
        "\n",
        "\n",
        "            return answer, \"\\n\\n\".join(sources)\n",
        "\n",
        "        except Exception as qa_e:\n",
        "            return f\"Error processing question with QA chain: {qa_e}\", \"\"\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        err = f\"An unexpected error occurred in answer_question: {e}\"\n",
        "        # Return the same error in both slots to display in UI\n",
        "        return err, err\n",
        "\n",
        "# --- Gradio Interface Definition ---\n",
        "\n",
        "gr.Interface(\n",
        "    fn=answer_question,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=2, placeholder=\"Ask a ServiceNow question via text...\", label=\"Text Input\"), # Text input\n",
        "        gr.Audio(sources=[\"microphone\"], type=\"filepath\", label=\"Ask your question via microphone\") # Audio input\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Answer\"),\n",
        "        gr.Textbox(label=\"Sources\")\n",
        "    ],\n",
        "    title=\"ServiceNow QA Assistant\",\n",
        "    description=\"RAG-powered assistant over your ServiceNow YouTube transcripts with text and speech input.\",\n",
        "    allow_flagging=\"never\" # Use flagging_mode instead in newer Gradio versions\n",
        ").launch()\n",
        "'''\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(code)"
      ],
      "metadata": {
        "id": "ulPNw3nGnzHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQ7nGxV_XBVs"
      },
      "outputs": [],
      "source": [
        "requirements = '''gradio\n",
        "langchain\n",
        "langchain-openai\n",
        "langchain-community\n",
        "langchain-core\n",
        "sentence-transformers\n",
        "transformers\n",
        "faiss-cpu\n",
        "openai\n",
        "yt-dlp\n",
        "python-dotenv\n",
        "'''\n",
        "with open(\"requirements.txt\", \"w\") as f:\n",
        "    f.write(requirements)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npGn2Ts2XDBD"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(\"app.py\")\n",
        "files.download(\"requirements.txt\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "63471d69d3074ab3bc7fe17858527a87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_83a7fb19f9a840cb999a11aa29a660ab",
              "IPY_MODEL_a5d0db4ef85d415bafd7f141f992b964",
              "IPY_MODEL_f8ff5599aca9402e93231b4b201aec56"
            ],
            "layout": "IPY_MODEL_5159975a768d4698a9f315f24901fa9b"
          }
        },
        "83a7fb19f9a840cb999a11aa29a660ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc9ec0642b974aca9a17f8a292844aa6",
            "placeholder": "​",
            "style": "IPY_MODEL_b391aa9502e244cba111b56543665617",
            "value": "Chunking transcripts: "
          }
        },
        "a5d0db4ef85d415bafd7f141f992b964": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23d55aad06f2423bbaa71e6e338dd3f5",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b2381747d1a14e7cac15623809c96061",
            "value": 1
          }
        },
        "f8ff5599aca9402e93231b4b201aec56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1073e3fd4ea74f28b019958f7fc36de8",
            "placeholder": "​",
            "style": "IPY_MODEL_00da92926f3a49fe85d66b47f86e88e0",
            "value": " 21/? [00:00&lt;00:00, 52.08it/s]"
          }
        },
        "5159975a768d4698a9f315f24901fa9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc9ec0642b974aca9a17f8a292844aa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b391aa9502e244cba111b56543665617": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23d55aad06f2423bbaa71e6e338dd3f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "b2381747d1a14e7cac15623809c96061": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1073e3fd4ea74f28b019958f7fc36de8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00da92926f3a49fe85d66b47f86e88e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}