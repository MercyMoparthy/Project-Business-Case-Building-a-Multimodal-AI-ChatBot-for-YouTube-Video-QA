{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84c9e2a5",
   "metadata": {},
   "source": [
    "## Model: distilgpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3ab7901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "from pinecone import Pinecone\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d77e64",
   "metadata": {},
   "source": [
    "verify Pinecone integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ee1f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 14:13:27,125 - INFO - Total vectors in index youtube-transcripts: 615\n",
      "2025-06-26 14:13:27,125 - INFO - Verification successful: 615 vectors match expected count 615\n",
      "2025-06-26 14:13:27,326 - INFO - Found vector for chunk_id 1_0\n",
      "2025-06-26 14:13:27,528 - INFO - Found vector for chunk_id 2_0\n",
      "2025-06-26 14:13:27,707 - INFO - Found vector for chunk_id 3_0\n",
      "2025-06-26 14:13:27,887 - INFO - Found vector for chunk_id 4_0\n",
      "2025-06-26 14:13:28,065 - INFO - Found vector for chunk_id 5_0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "615"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "from pinecone import Pinecone\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import pipeline\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def verify_pinecone_vectors(index_name='youtube-transcripts', expected_count=615):\n",
    "    \"\"\"Verify vectors in Pinecone index.\"\"\"\n",
    "    load_dotenv()\n",
    "    pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "    if not pinecone_api_key:\n",
    "        logging.error('PINECONE_API_KEY not found in .env')\n",
    "        raise ValueError('PINECONE_API_KEY not found')\n",
    "\n",
    "    # Initialize Pinecone\n",
    "    pc = Pinecone(api_key=pinecone_api_key)\n",
    "    if index_name not in [idx['name'] for idx in pc.list_indexes()]:\n",
    "        logging.error(f'Index {index_name} not found')\n",
    "        raise ValueError(f'Index {index_name} not found')\n",
    "\n",
    "    # Connect to index\n",
    "    index = pc.Index(index_name)\n",
    "    \n",
    "    # Get index stats\n",
    "    stats = index.describe_index_stats()\n",
    "    total_vectors = stats['total_vector_count']\n",
    "    \n",
    "    # Verify vector count\n",
    "    logging.info(f'Total vectors in index {index_name}: {total_vectors}')\n",
    "    if total_vectors == expected_count:\n",
    "        logging.info(f'Verification successful: {total_vectors} vectors match expected count {expected_count}')\n",
    "    else:\n",
    "        logging.warning(f'Verification failed: Found {total_vectors} vectors, expected {expected_count}')\n",
    "\n",
    "    # Sample a few vector IDs to confirm format\n",
    "    sample_ids = [f'{i}_0' for i in range(1, 6)]  # Check first chunk of videos 1-5\n",
    "    for sample_id in sample_ids:\n",
    "        try:\n",
    "            result = index.fetch(ids=[sample_id])\n",
    "            if sample_id in result['vectors']:\n",
    "                logging.info(f'Found vector for chunk_id {sample_id}')\n",
    "            else:\n",
    "                logging.warning(f'No vector found for chunk_id {sample_id}')\n",
    "        except Exception as e:\n",
    "            logging.error(f'Error fetching chunk_id {sample_id}: {e}')\n",
    "\n",
    "    return total_vectors\n",
    "\n",
    "# Run verification\n",
    "verify_pinecone_vectors()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3063b6af",
   "metadata": {},
   "source": [
    "Implement Retrieval-Augmented Generation (RAG) Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98469973",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def retrieve_chunks(query, index_name='youtube-transcripts', top_k=5):\n",
    "    \"\"\"Retrieve top-k transcript chunks from Pinecone.\"\"\"\n",
    "    load_dotenv()\n",
    "    pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "    if not pinecone_api_key:\n",
    "        logging.error('PINECONE_API_KEY not found in .env')\n",
    "        raise ValueError('PINECONE_API_KEY not found')\n",
    "\n",
    "    # Initialize Pinecone and SentenceTransformer\n",
    "    pc = Pinecone(api_key=pinecone_api_key)\n",
    "    index = pc.Index(index_name)\n",
    "    embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    # Encode query\n",
    "    query_embedding = embedder.encode(query, show_progress_bar=False).tolist()\n",
    "\n",
    "    # Query Pinecone\n",
    "    results = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n",
    "    chunks = [match['metadata']['text'] for match in results['matches']]\n",
    "    \n",
    "    logging.info(f'Retrieved {len(chunks)} chunks for query: {query}')\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c11a0843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 14:13:28,136 - INFO - Use pytorch device_name: cpu\n",
      "2025-06-26 14:13:28,137 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-06-26 14:13:30,870 - INFO - Retrieved 5 chunks for query: What is an AI agent in ServiceNow?\n",
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "2025-06-26 14:13:46,144 - INFO - Generated response for query: What is an AI agent in ServiceNow?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is an AI agent in ServiceNow?\n",
      "Response: that we are going to go the other direction, maybe you can have some kind of ai agent in your team to help you figure out what to do with that.\n",
      "So let me just say that you are interested in working with ai agency in this area and you are a really strong supporter of the A-Team. There are a number of organizations that have been involved with the A-Team and we all want to see some of our new A-Team members and they are very excited to work with us.\n",
      "So it is truly interesting to see how this can be a really good way for us to get more people to come and work with us.\n",
      "Now that we have been getting a lot of people involved in the A-Team there is a lot of interest in the A-Team. We have the backing of a few people who are really passionate about this and we are looking forward to working with them as well.\n",
      "The A-Team has been working on a lot of A-Team members and it seems to be the very first step in understanding the A-Team and the A-Team and that's the way the A-Team will be going.\n",
      "So now that we have a lot of people interested in working with us we are looking\n"
     ]
    }
   ],
   "source": [
    "def generate_response(query, chunks):\n",
    "    \"\"\"Generate response using retrieved chunks and a lightweight LLM.\"\"\"\n",
    "    # Combine chunks into context\n",
    "    context = \"\\n\".join(chunks) if chunks else \"No relevant information found.\"\n",
    "    \n",
    "    # Define prompt template\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"context\", \"query\"],\n",
    "        template=\"Based on the following context, answer the query concisely:\\nContext: {context}\\nQuery: {query}\\nAnswer:\"\n",
    "    )\n",
    "    \n",
    "    # Format prompt\n",
    "    prompt = prompt_template.format(context=context, query=query)\n",
    "    \n",
    "    # Placeholder for lightweight LLM (e.g., Grok via xAI API)\n",
    "    # Replace with actual API call if available, or use HuggingFace model\n",
    "    try:\n",
    "        llm = pipeline('text-generation', model='distilgpt2', device=-1)\n",
    "        response = llm(prompt, max_length=150, truncation=True, do_sample=True, num_return_sequences=1)[0]['generated_text']\n",
    "        answer = response.split('Answer:')[-1].strip() if 'Answer:' in response else response.strip()\n",
    "    except Exception as e:\n",
    "        logging.error(f'LLM request failed: {e}')\n",
    "        answer = \"Error generating response. Using context directly:\\n\" + context[:200]\n",
    "    \n",
    "    logging.info(f'Generated response for query: {query}')\n",
    "    return answer\n",
    "\n",
    "# Example usage\n",
    "query = \"What is an AI agent in ServiceNow?\"\n",
    "chunks = retrieve_chunks(query)\n",
    "response = generate_response(query, chunks)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3266de90",
   "metadata": {},
   "source": [
    "Built Chatbot interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98557afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 14:14:57,731 - INFO - Use pytorch device_name: cpu\n",
      "2025-06-26 14:14:57,734 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 14:14:59,910 - INFO - Retrieved 5 chunks for query: What is ITSM in ServiceNow?\n",
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "2025-06-26 14:15:13,096 - INFO - Generated response for query: What is ITSM in ServiceNow?\n",
      "2025-06-26 14:15:13,147 - INFO - Use pytorch device_name: cpu\n",
      "2025-06-26 14:15:13,148 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is ITSM in ServiceNow?\n",
      "A: it is a command line system and it is easy to use for this.\n",
      "So if you want to use service operation work face if you want to use service operation work face if you want to use service operation work face if you would like to use service operation work face if you would like to use service operation work face if you would like to use service operation work face if you would like to use service operation work face if you would like to use service operation work face if you would like to use service operation work face if you would like to use service operation work face if you would like to use service operation work face if you would like to use service operation work face if you would like to use service operation work face if you would like to use service operation work face if you would like to use service operation work face if you would like to use service operation work face if you would like to use service operation work face if you would like to use service operation work face if you would like to use service operation work face if you would like to use service operation work face if you would like to use service operation work face if you would like to use service operation work face if you would like to use service operation work face if you would like to use service operation work face if you\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 14:15:15,839 - INFO - Retrieved 5 chunks for query: Explain CMDB relationships.\n",
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "2025-06-26 14:15:30,655 - INFO - Generated response for query: Explain CMDB relationships.\n",
      "2025-06-26 14:15:30,704 - INFO - Use pytorch device_name: cpu\n",
      "2025-06-26 14:15:30,705 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Explain CMDB relationships.\n",
      "A: b this approach can still work right i mean they dont have to be bound down to say that you know i need to get my cmdb like 70 to ready state exactly it pretty much decoupled yeah and i think i mentioned this very briefly in the beginning that whole data strategy the service now ha around work for data fabric and the rapridb which is kind of pretty advanced way of ive been talking about it this past week, i think i mentioned this very briefly in the beginning that whole data strategy the service now ha around work for data fabric and the rapridb which is kind of pretty advanced way of ive been talking about it this past week, i think i mentioned this very briefly in the beginning that whole data strategy the service now ha around work for data fabric and the rapridb which is kind of pretty advanced way of ive been talking about it this past week, i think i mentioned this very briefly in the beginning that whole data strategy the service now ha around work for data fabric and the rapridb which is kind of pretty advanced way of ive been talking about it this past week, i think i mentioned this very briefly in the beginning that whole data strategy the service now ha around work for data fabric and the rapridb which is\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 14:15:33,689 - INFO - Retrieved 5 chunks for query: How does Incident Management work?\n",
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "2025-06-26 14:15:48,844 - INFO - Generated response for query: How does Incident Management work?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: How does Incident Management work?\n",
      "A: knowledge that they already have.\n",
      "You also mentioned that a local solution which uses a local service provider with the same customer base is a great solution that can be used for multiple business processes.\n",
      "To summarize, this solution will be more than just one. It will be a great solution that will solve the biggest problem of all for you and your team and the team. You will continue to be able to get the whole team involved and if you can find something that will help you get the whole business involved and if you can help with the solution, you can be the best solution in the world.\n",
      "Now that youâ€¡ have been through that journey and you are ready to head back to your normal role of business management you will be looking back to the work you did for the first time, your next step is to get started and start looking forward to making your work more manageable.\n",
      "Now that you have been through that journey and you are ready to head back to your normal role of business management you will be looking back to the work you did for the first time, your next step is to get started and start looking forward to making your work more manageable.\n",
      "You are about to be able to get your work done, all in one step. You are about to\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_queries = [\n",
    "    \"What is ITSM in ServiceNow?\",\n",
    "    \"Explain CMDB relationships.\",\n",
    "    \"How does Incident Management work?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    chunks = retrieve_chunks(query)\n",
    "    result = generate_response(query, chunks)\n",
    "    print(f\"Q: {query}\\nA: {result}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
