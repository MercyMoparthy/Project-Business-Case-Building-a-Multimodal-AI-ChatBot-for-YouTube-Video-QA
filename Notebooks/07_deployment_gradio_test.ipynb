{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Tl5t2-ViO2YV"
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain langchain-community langchain-openai sentence-transformers faiss-cpu gradio --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LzwwQwQHRIt8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_openai import OpenAI\n",
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H45TkcP9CRAZ",
        "outputId": "3c2a86f3-db33-4139-fc29-0007dfb9d0f3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xLmJbIPRRNuo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "aa363f76728248628c159b506d3766af",
            "4875219ab2134728aca60503811214b9",
            "8dfea77f29e3418a9b2d685e95790e6e",
            "c22eb13c6a4249368b743de574109e7a",
            "acd9f4dc814d4de3a13129d7107cbf9a",
            "53a3be5e2de94a54a1b180d4f915fb14",
            "3ce5620468a741afa95876e825afc105",
            "8a3d4df32b574bdf97973c9e23ef6c70",
            "ea56ccab75c74bac98b62fc7f0996e8e",
            "64f1808a5c264215bea3bdf8a08cd152",
            "238756c1aa734f91b2ec6dc0b1aa6f72"
          ]
        },
        "outputId": "13597386-e3ea-4487-ba37-f61e7898311e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Chunking transcripts: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa363f76728248628c159b506d3766af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Loaded and chunked 656 document chunks from /content/drive/MyDrive/ServiceNow_Audio_Transcripts.\n"
          ]
        }
      ],
      "source": [
        "transcript_folder = Path(\"/content/drive/MyDrive/ServiceNow_Audio_Transcripts\")\n",
        "transcript_folder.mkdir(parents=True, exist_ok=True)\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "\n",
        "documents: list[Document] = []\n",
        "for txt_path in tqdm(transcript_folder.glob(\"*.txt\"), desc=\"Chunking transcripts\"):\n",
        "    raw_text = txt_path.read_text(encoding=\"utf-8\")\n",
        "    for chunk in splitter.split_text(raw_text):\n",
        "        documents.append(\n",
        "            Document(page_content=chunk, metadata={\"source\": txt_path.name})\n",
        "        )\n",
        "\n",
        "# 4. Summary\n",
        "print(f\"âœ… Loaded and chunked {len(documents)} document chunks from {transcript_folder}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWl-S-b2RPuJ",
        "outputId": "95db083a-45a1-4976-f92f-3f653e616c64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-5-2449386041.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
          ]
        }
      ],
      "source": [
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "faiss_dir = \"/content/drive/MyDrive/faiss_store\"\n",
        "faiss_dir = Path(faiss_dir)\n",
        "Path(faiss_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "faiss_index = FAISS.load_local(\n",
        "    faiss_dir,\n",
        "    embedding_model,\n",
        "    allow_dangerous_deserialization=True\n",
        ")\n",
        "\n",
        "# 4. Now you can query it or wrap it in a retriever/chainâ€”no save_local() needed here\n",
        "retriever = faiss_index.as_retriever(search_kwargs={\"k\": 5})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KfvjXdnkRRbL"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(\n",
        "    temperature=0,\n",
        "    openai_api_key=userdata.get(\"OPENAI_API_KEY\")\n",
        ")\n",
        "\n",
        "retriever = faiss_index.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 5}\n",
        ")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gRsN5x83PJNQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd3ef8c0-d26c-491b-b028-c2fb17b0312f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "from typing import Tuple, Union\n",
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the speech-to-text model (changed to Whisper small)\n",
        "transcriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-small\")\n",
        "\n",
        "def answer_question(text_input: str, audio_input: Union[str, dict, None]) -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Runs the RetrievalQA chain and formats the answer along with source snippets.\n",
        "    Handles both text and audio input.\n",
        "\n",
        "    Args:\n",
        "        text_input: The string input from the text box.\n",
        "        audio_input: The audio input, expected as a string filepath from Gradio.\n",
        "\n",
        "    Returns:\n",
        "      - answer text\n",
        "      - formatted source list (one entry per document)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        question = \"\"\n",
        "        print(f\"Debug: text_input type: {type(text_input)}, value: '{text_input}'\")\n",
        "        print(f\"Debug: audio_input type: {type(audio_input)}, value: {audio_input}\")\n",
        "\n",
        "\n",
        "        # Process audio input if provided (handle as a string filepath)\n",
        "        if isinstance(audio_input, str) and audio_input:\n",
        "             audio_filepath = audio_input\n",
        "             print(f\"Debug: Processing audio from filepath: {audio_filepath}\")\n",
        "             try:\n",
        "                 transcription_result = transcriber(audio_filepath)\n",
        "                 print(f\"Debug: Transcription result: {transcription_result}\")\n",
        "                 question = transcription_result.get(\"text\", \"\")\n",
        "                 if not question:\n",
        "                     print(\"Debug: Transcription resulted in empty string.\")\n",
        "                     return \"Could not transcribe audio. Please try again.\", \"\"\n",
        "                 question = question.strip() # Remove leading/trailing whitespace from transcription\n",
        "                 if not question:\n",
        "                     print(\"Debug: Stripped transcription is empty.\")\n",
        "                     return \"Transcribed audio was empty. Please try again.\", \"\"\n",
        "             except Exception as trans_e:\n",
        "                 print(f\"Debug: Error during transcription: {trans_e}\")\n",
        "                 return f\"Error during transcription: {trans_e}\", \"\"\n",
        "\n",
        "\n",
        "        # Use text input if audio was not provided or transcription failed\n",
        "        if not question and isinstance(text_input, str) and text_input.strip():\n",
        "            print(\"Debug: Using text input.\")\n",
        "            question = text_input.strip()\n",
        "            if not question:\n",
        "                print(\"Debug: Stripped text input is empty.\")\n",
        "                return \"Please enter a question or record audio.\", \"\"\n",
        "\n",
        "        # Ensure there is a question to process after handling input types\n",
        "        if not question:\n",
        "             print(\"Debug: Final question is empty after checking both inputs.\")\n",
        "             return \"No valid question provided via text or audio.\", \"\"\n",
        "\n",
        "        print(f\"Debug: Final question for QA chain: '{question}'\")\n",
        "\n",
        "        # Run the QA chain with the processed question\n",
        "        try:\n",
        "            print(\"Debug: Calling qa_chain...\")\n",
        "            response = qa_chain(question)\n",
        "            print(f\"Debug: qa_chain response type: {type(response)}\")\n",
        "            print(f\"Debug: qa_chain response keys: {response.keys() if isinstance(response, dict) else 'Not a dict'}\")\n",
        "            answer = response.get(\"result\", \"\") # Use .get() with a default value\n",
        "            print(f\"Debug: Answer extracted: '{answer}')\")\n",
        "\n",
        "            # Build source list with preview snippets\n",
        "            sources = []\n",
        "            source_documents = response.get(\"source_documents\", []) # Use .get() with a default empty list\n",
        "            print(f\"Debug: Number of source documents: {len(source_documents)}\")\n",
        "            for doc in source_documents:\n",
        "                text = doc.page_content.strip()\n",
        "                preview = text[:400] + \"...\" if len(text) > 400 else text\n",
        "                sources.append(f\"ðŸ“„ {doc.metadata.get('source', 'unknown')}\\nðŸ”Ž {preview}\")\n",
        "            print(f\"Debug: Sources formatted. Number of formatted sources: {len(sources)}\")\n",
        "\n",
        "            if not answer:\n",
        "                 print(\"Debug: Answer is empty after qa_chain.\")\n",
        "                 # It's possible the QA chain returned successfully but with no relevant answer\n",
        "                 return \"Could not find a relevant answer in the documents.\", \"\\n\\n\".join(sources)\n",
        "\n",
        "\n",
        "            return answer, \"\\n\\n\".join(sources)\n",
        "\n",
        "        except Exception as qa_e:\n",
        "            print(f\"Debug: Error during qa_chain execution or response processing: {qa_e}\")\n",
        "            return f\"Error processing question with QA chain: {qa_e}\", \"\"\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        err = f\"An unexpected error occurred in answer_question: {e}\"\n",
        "        print(f\"Debug: Unexpected error in answer_question: {e}\") # Print error for debugging\n",
        "        # Return the same error in both slots to display in UI\n",
        "        return err, err"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4iJ04-EZWeW0"
      },
      "outputs": [],
      "source": [
        "code = '''import os\n",
        "from typing import Tuple, Union\n",
        "import gradio as gr\n",
        "from langchain_openai import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from pathlib import Path\n",
        "from transformers import pipeline\n",
        "from google.colab import userdata # Assuming you'll run this part in Colab to get the key\n",
        "\n",
        "# --- Configuration ---\n",
        "# You might need to securely load this in a production app (e.g., using environment variables)\n",
        "# For Colab, you can get it from userdata secrets\n",
        "openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "if not openai_api_key:\n",
        "    raise EnvironmentError(\"OPENAI_API_KEY not set. Please add it to Colab secrets.\")\n",
        "\n",
        "# Define the path to your saved FAISS index\n",
        "faiss_store = \"/content/drive/MyDrive/faiss_store\"\n",
        "if not Path(faiss_store).exists():\n",
        "     raise FileNotFoundError(f\"FAISS index not found at {faiss_store}. Please ensure it's saved there.\")\n",
        "\n",
        "# --- Model and Chain Initialization ---\n",
        "\n",
        "# Load the speech-to-text model\n",
        "transcriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-small\")\n",
        "\n",
        "# Initialize the embedding model (used for loading FAISS)\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Load the FAISS index from the local directory\n",
        "faiss_index = FAISS.load_local(\n",
        "    faiss_store,\n",
        "    embedding_model,\n",
        "    allow_dangerous_deserialization=True # Be cautious with untrusted sources\n",
        ")\n",
        "\n",
        "# Initialize the OpenAI LLM\n",
        "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
        "\n",
        "# Create the RetrievalQA chain\n",
        "retriever = faiss_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "# --- Gradio Interface Function ---\n",
        "\n",
        "def answer_question(text_input: str, audio_input: Union[str, dict, None]) -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Runs the RetrievalQA chain and formats the answer along with source snippets.\n",
        "    Handles both text and audio input.\n",
        "\n",
        "    Args:\n",
        "        text_input: The string input from the text box.\n",
        "        audio_input: The audio input, expected as a string filepath from Gradio.\n",
        "\n",
        "    Returns:\n",
        "      - answer text\n",
        "      - formatted source list (one entry per document)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        question = \"\"\n",
        "\n",
        "        # Process audio input if provided (handle as a string filepath)\n",
        "        if isinstance(audio_input, str) and audio_input:\n",
        "             audio_filepath = audio_input\n",
        "             try:\n",
        "                 transcription_result = transcriber(audio_filepath)\n",
        "                 question = transcription_result.get(\"text\", \"\")\n",
        "                 if not question:\n",
        "                     return \"Could not transcribe audio. Please try again.\", \"\"\n",
        "                 question = question.strip() # Remove leading/trailing whitespace from transcription\n",
        "                 if not question:\n",
        "                     return \"Transcribed audio was empty. Please try again.\", \"\"\n",
        "             except Exception as trans_e:\n",
        "                 return f\"Error during transcription: {trans_e}\", \"\"\n",
        "\n",
        "\n",
        "        # Use text input if audio was not provided or transcription failed\n",
        "        if not question and isinstance(text_input, str) and text_input.strip():\n",
        "            question = text_input.strip()\n",
        "            if not question:\n",
        "                return \"Please enter a question or record audio.\", \"\"\n",
        "\n",
        "        # Ensure there is a question to process after handling input types\n",
        "        if not question:\n",
        "             return \"No valid question provided via text or audio.\", \"\"\n",
        "\n",
        "        # Run the QA chain with the processed question\n",
        "        try:\n",
        "            response = qa_chain(question)\n",
        "            answer = response.get(\"result\", \"\") # Use .get() with a default value\n",
        "\n",
        "            # Build source list with preview snippets\n",
        "            sources = []\n",
        "            source_documents = response.get(\"source_documents\", []) # Use .get() with a default empty list\n",
        "            for doc in source_documents:\n",
        "                text = doc.page_content.strip()\n",
        "                preview = text[:400] + \"...\" if len(text) > 400 else text\n",
        "                sources.append(f\"ðŸ“„ {doc.metadata.get('source', 'unknown')}\\nðŸ”Ž {preview}\")\n",
        "\n",
        "            if not answer:\n",
        "                 # It's possible the QA chain returned successfully but with no relevant answer\n",
        "                 return \"Could not find a relevant answer in the documents.\", \"\\n\\n\".join(sources)\n",
        "\n",
        "\n",
        "            return answer, \"\\n\\n\".join(sources)\n",
        "\n",
        "        except Exception as qa_e:\n",
        "            return f\"Error processing question with QA chain: {qa_e}\", \"\"\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        err = f\"An unexpected error occurred in answer_question: {e}\"\n",
        "        # Return the same error in both slots to display in UI\n",
        "        return err, err\n",
        "\n",
        "# --- Gradio Interface Definition ---\n",
        "\n",
        "gr.Interface(\n",
        "    fn=answer_question,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=2, placeholder=\"Ask a ServiceNow question via text...\", label=\"Text Input\"), # Text input\n",
        "        gr.Audio(sources=[\"microphone\"], type=\"filepath\", label=\"Ask your question via microphone\") # Audio input\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Answer\"),\n",
        "        gr.Textbox(label=\"Sources\")\n",
        "    ],\n",
        "    title=\"ServiceNow QA Assistant\",\n",
        "    description=\"RAG-powered assistant over your ServiceNow YouTube transcripts with text and speech input.\",\n",
        "    allow_flagging=\"never\" # Use flagging_mode instead in newer Gradio versions\n",
        ").launch()\n",
        "'''\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HQ7nGxV_XBVs"
      },
      "outputs": [],
      "source": [
        "requirements = '''gradio\n",
        "langchain\n",
        "langchain-openai\n",
        "langchain-community\n",
        "langchain-core\n",
        "sentence-transformers\n",
        "transformers\n",
        "faiss-cpu\n",
        "openai\n",
        "yt-dlp\n",
        "python-dotenv\n",
        "'''\n",
        "with open(\"requirements.txt\", \"w\") as f:\n",
        "    f.write(requirements)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "npGn2Ts2XDBD",
        "outputId": "2a32418e-0c25-453e-ade4-624fa02875c7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_68870520-ac1c-49f7-b02b-b18c0c17e616\", \"app.py\", 5408)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_106398db-70ed-46f0-88c0-666b254eeb02\", \"requirements.txt\", 142)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download(\"app.py\")\n",
        "files.download(\"requirements.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOrlSUe1POKJ",
        "outputId": "14eb2895-2618-480c-adcf-46c3bb86b908"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/interface.py:419: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://c6b2ea89c1c2a30524.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://c6b2ea89c1c2a30524.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "gr.Interface(\n",
        "    fn=answer_question,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=2, placeholder=\"Ask a ServiceNow question via text...\", label=\"Text Input\"), # Text input\n",
        "        gr.Audio(sources=[\"microphone\"], type=\"filepath\", label=\"Ask your question via microphone\") # Audio input\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Answer\"),\n",
        "        gr.Textbox(label=\"Sources\")\n",
        "    ],\n",
        "    title=\"ServiceNow QA Assistant\",\n",
        "    description=\"RAG-powered assistant over your ServiceNow YouTube transcripts with text and speech input.\", # Updated description\n",
        "    allow_flagging=\"never\"\n",
        ").launch(\n",
        "    share=True,\n",
        "    debug=True,\n",
        "    inline=False,\n",
        "    show_error=True,\n",
        "    prevent_thread_lock=True\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "aa363f76728248628c159b506d3766af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4875219ab2134728aca60503811214b9",
              "IPY_MODEL_8dfea77f29e3418a9b2d685e95790e6e",
              "IPY_MODEL_c22eb13c6a4249368b743de574109e7a"
            ],
            "layout": "IPY_MODEL_acd9f4dc814d4de3a13129d7107cbf9a"
          }
        },
        "4875219ab2134728aca60503811214b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53a3be5e2de94a54a1b180d4f915fb14",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3ce5620468a741afa95876e825afc105",
            "value": "Chunkingâ€‡transcripts:â€‡"
          }
        },
        "8dfea77f29e3418a9b2d685e95790e6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a3d4df32b574bdf97973c9e23ef6c70",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ea56ccab75c74bac98b62fc7f0996e8e",
            "value": 1
          }
        },
        "c22eb13c6a4249368b743de574109e7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64f1808a5c264215bea3bdf8a08cd152",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_238756c1aa734f91b2ec6dc0b1aa6f72",
            "value": "â€‡21/?â€‡[00:00&lt;00:00,â€‡58.22it/s]"
          }
        },
        "acd9f4dc814d4de3a13129d7107cbf9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53a3be5e2de94a54a1b180d4f915fb14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ce5620468a741afa95876e825afc105": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a3d4df32b574bdf97973c9e23ef6c70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "ea56ccab75c74bac98b62fc7f0996e8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "64f1808a5c264215bea3bdf8a08cd152": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "238756c1aa734f91b2ec6dc0b1aa6f72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}