{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca79a96b",
   "metadata": {},
   "source": [
    "## Data Processing & Embedding\n",
    "### > Data PreProcessing - Cleaning, Chunking\n",
    "* Lemmatization & Tokenization\n",
    "* Normalizing the columns\n",
    "* Splitting the Chunks\n",
    "* Loading and Preprocessing transcripts\n",
    "* Converting to LangChain docs and Split Chunks\n",
    "* Saving Processed Data as processed_transcripts.csv file\n",
    "* Validating creating Data validation file as validation_report.txt\n",
    "### > Vector and Embedding\n",
    "* Vectorizing using FAISS\n",
    "* Embedding with all-MiniLM-L6-v2 Model\n",
    "* Log Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4972e323",
   "metadata": {},
   "source": [
    "## Step1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5f15138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mercy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import logging\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8612cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_file = '../data/ServiceNow_Youtube_Metadata_Clean.csv'\n",
    "transcript_file = '../data/video_metadata_with_transcripts.csv'\n",
    "output_file = '../data/processed_transcripts.csv'\n",
    "validation_report = '../logs/validation_report.txt'\n",
    "\n",
    "for file in [metadata_file, transcript_file]:\n",
    "    if not os.path.exists(file):\n",
    "        logging.error(f'File not found: {file}')\n",
    "        raise FileNotFoundError(f'File not found: {file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a67f5094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path().resolve().parent\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "LOG_DIR = BASE_DIR / 'logs'\n",
    "FAISS_DIR = BASE_DIR / \"faiss_store\"\n",
    "\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FAISS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b38eb36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata columns: ['Number', 'Youtube_link', 'Subject', 'title', 'channel', 'description', 'length', 'publish_date', 'views', 'error']\n",
      "Transcript columns: ['Number', 'Youtube_link', 'Subject', 'title', 'channel', 'description', 'length', 'publish_date', 'views', 'error', 'transcript']\n"
     ]
    }
   ],
   "source": [
    "df_meta = pd.read_csv('../data/ServiceNow_Youtube_Metadata_Clean.csv')\n",
    "print(\"Metadata columns:\", df_meta.columns.tolist())\n",
    "\n",
    "df_trans = pd.read_csv('../data/video_metadata_with_transcripts.csv')\n",
    "print(\"Transcript columns:\", df_trans.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10693cd",
   "metadata": {},
   "source": [
    "## Step2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6decc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 328 document chunks.\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        logging.warning('Empty text encountered in clean_text')\n",
    "        return ''\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text.lower().strip()\n",
    "\n",
    "def tokenize_and_lemmatize(text):\n",
    "    if not text:\n",
    "        logging.warning('Empty text encountered in tokenize_and_lemmatize')\n",
    "        return ''\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens if token]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "def load_and_process_transcripts(transcript_path):\n",
    "    df = pd.read_csv(transcript_path)\n",
    "    \n",
    "    df = df.rename(columns={\n",
    "        'Youtube_link': 'youtube_link',\n",
    "        'Number': 'video_id',\n",
    "        'Subject': 'subject',\n",
    "        'title': 'title',\n",
    "        'transcript': 'transcript'\n",
    "    })\n",
    "    \n",
    "    df['cleaned_transcript'] = df['transcript'].apply(clean_text).apply(tokenize_and_lemmatize)\n",
    "    \n",
    "    logging.info(f'Loaded and processed {len(df)} transcript records')\n",
    "    return df\n",
    "\n",
    "def prepare_langchain_docs(df):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "    docs = []\n",
    "    missing_transcripts = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        if not row['cleaned_transcript']:\n",
    "            missing_transcripts.append(row['video_id'])\n",
    "            logging.warning(f'Missing transcript for video_id {row[\"video_id\"]}')\n",
    "            continue\n",
    "        \n",
    "        metadata = {\n",
    "            'title': row['title'],\n",
    "            'url': row['youtube_link'],\n",
    "            'subject': row['subject'],\n",
    "            'video_id': row['video_id'],\n",
    "        }\n",
    "        \n",
    "        chunks = text_splitter.split_text(row['cleaned_transcript'])\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            doc = Document(page_content=chunk, metadata={**metadata, 'chunk_id': f\"{row['video_id']}_{i}\"})\n",
    "            docs.append(doc)\n",
    "            \n",
    "        logging.info(f'Created {len(chunks)} chunks for video_id {row[\"video_id\"]}')\n",
    "        \n",
    "    logging.info(f'Created {len(docs)} total LangChain documents')\n",
    "    return docs, missing_transcripts\n",
    "\n",
    "df_clean = load_and_process_transcripts(transcript_file)\n",
    "langchain_docs, missing = prepare_langchain_docs(df_clean)\n",
    "\n",
    "print(f\"Processed {len(langchain_docs)} document chunks.\")\n",
    "if missing:\n",
    "    print(f\"Missing transcripts for video IDs: {missing}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52485296",
   "metadata": {},
   "source": [
    "## Step2.1: Preprocess Transcripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1193125e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata success: 21/22\n",
      "Missing transcripts: 0/22\n",
      "Processed chunks: 328\n",
      "   video_id chunk_id                                               text  \\\n",
      "0         1      1_0  hey folk how you doing chris thanky here and i...   \n",
      "1         1      1_1  and issue and in those project youre going to ...   \n",
      "2         1      1_2  project the project status report any issue th...   \n",
      "3         1      1_3  be it would be crazy right it would take you f...   \n",
      "4         1      1_4  person and asking him this question and asking...   \n",
      "\n",
      "                                             subject  \n",
      "0  An AI Agent that knows everything about your P...  \n",
      "1  An AI Agent that knows everything about your P...  \n",
      "2  An AI Agent that knows everything about your P...  \n",
      "3  An AI Agent that knows everything about your P...  \n",
      "4  An AI Agent that knows everything about your P...  \n"
     ]
    }
   ],
   "source": [
    "def preprocess_transcripts():\n",
    "    df_clean = load_and_process_transcripts(transcript_file)\n",
    "    langchain_docs, missing_transcripts = prepare_langchain_docs(df_clean)\n",
    "    processed_data = []\n",
    "    for doc in langchain_docs:\n",
    "        processed_data.append({\n",
    "            'video_id': doc.metadata['video_id'],    \n",
    "            'chunk_id': doc.metadata['chunk_id'],\n",
    "            'text': doc.page_content,\n",
    "            'subject': doc.metadata['subject']\n",
    "        })\n",
    "    df_processed = pd.DataFrame(processed_data)\n",
    "    if df_processed.empty:\n",
    "        logging.error('No processed data generated')\n",
    "        raise ValueError('No processed data generated')\n",
    "    try:\n",
    "        df_processed.to_csv(output_file, index=False, encoding='utf-8')\n",
    "        logging.info(f'Saved {len(df_processed)} transcript chunks to {output_file}')\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error saving processed data: {e}')\n",
    "        raise\n",
    "    df_transcripts = pd.read_csv(transcript_file)\n",
    "    metadata_success = df_transcripts['title'].notnull().sum()\n",
    "    missing_trans = df_transcripts['transcript'].isna().sum()\n",
    "    total_chunks = len(df_processed)\n",
    "    sample_chunk = df_processed['text'].iloc[0] if not df_processed.empty else 'No chunks'\n",
    "    with open(validation_report, 'w') as f:\n",
    "        f.write(f'Transcripts: {len(df_transcripts)} videos, {missing_trans} missing\\n')\n",
    "        f.write(f'Processed Chunks: {total_chunks}\\n')\n",
    "        f.write(f'Sample Chunk:\\n{sample_chunk}\\n')\n",
    "        if missing_transcripts:\n",
    "            f.write(f'Missing transcripts for video IDs: {missing_transcripts}\\n')\n",
    "    logging.info(f'Validation report saved to {validation_report}')\n",
    "    print(f'Metadata success: {metadata_success}/{len(df_transcripts)}')\n",
    "    print(f'Missing transcripts: {missing_trans}/{len(df_transcripts)}')\n",
    "    print(f'Processed chunks: {total_chunks}')\n",
    "    if missing_transcripts:\n",
    "        print(f'Missing transcripts for video IDs: {missing_transcripts}')\n",
    "    print(df_processed.head())\n",
    "\n",
    "preprocess_transcripts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24af8586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 328\n",
      "Unique videos: 22\n",
      "   video_id chunk_id                                               text  \\\n",
      "0         1      1_0  hey folk how you doing chris thanky here and i...   \n",
      "1         1      1_1  and issue and in those project youre going to ...   \n",
      "2         1      1_2  project the project status report any issue th...   \n",
      "3         1      1_3  be it would be crazy right it would take you f...   \n",
      "4         1      1_4  person and asking him this question and asking...   \n",
      "\n",
      "                                             subject  \n",
      "0  An AI Agent that knows everything about your P...  \n",
      "1  An AI Agent that knows everything about your P...  \n",
      "2  An AI Agent that knows everything about your P...  \n",
      "3  An AI Agent that knows everything about your P...  \n",
      "4  An AI Agent that knows everything about your P...  \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_processed = pd.read_csv('../data/processed_transcripts.csv')\n",
    "    print(f'Total chunks: {len(df_processed)}')\n",
    "    print(f'Unique videos: {df_processed[\"video_id\"].nunique()}')\n",
    "    print(df_processed[['video_id', 'chunk_id', 'text', 'subject']].head())\n",
    "except FileNotFoundError:\n",
    "    logging.error(f'Output file not found: {output_file}')\n",
    "    print(f'Output file not found: {output_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f01d81",
   "metadata": {},
   "source": [
    "## Step2.2: Cleaning Processed Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345e1dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>subject</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chunk_0</td>\n",
       "      <td>1</td>\n",
       "      <td>An AI Agent that knows everything about your P...</td>\n",
       "      <td>and to you where i plan to show you the greate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chunk_1</td>\n",
       "      <td>1</td>\n",
       "      <td>An AI Agent that knows everything about your P...</td>\n",
       "      <td>and issue and in those project youre going to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chunk_2</td>\n",
       "      <td>1</td>\n",
       "      <td>An AI Agent that knows everything about your P...</td>\n",
       "      <td>project the project status report any issue th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chunk_3</td>\n",
       "      <td>1</td>\n",
       "      <td>An AI Agent that knows everything about your P...</td>\n",
       "      <td>be it would be crazy right it would take you f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chunk_4</td>\n",
       "      <td>1</td>\n",
       "      <td>An AI Agent that knows everything about your P...</td>\n",
       "      <td>person and asking him this question and asking...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  chunk_id  video_id                                            subject  \\\n",
       "0  chunk_0         1  An AI Agent that knows everything about your P...   \n",
       "1  chunk_1         1  An AI Agent that knows everything about your P...   \n",
       "2  chunk_2         1  An AI Agent that knows everything about your P...   \n",
       "3  chunk_3         1  An AI Agent that knows everything about your P...   \n",
       "4  chunk_4         1  An AI Agent that knows everything about your P...   \n",
       "\n",
       "                                                text  \n",
       "0  and to you where i plan to show you the greate...  \n",
       "1  and issue and in those project youre going to ...  \n",
       "2  project the project status report any issue th...  \n",
       "3  be it would be crazy right it would take you f...  \n",
       "4  person and asking him this question and asking...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "df = pd.read_csv(\"../Data/processed_transcripts.csv\")\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    filler_patterns = [\n",
    "        r\"\\b(hi|hey|hello|folks|how you doing|thank you|thanks|welcome|today|i am|i'm|this is .*?)\\b\",\n",
    "        r\"\\b(thanky here|and i\\b|let's talk about|so today|in this video)\\b\",\n",
    "        r\"\\b(folk|id like|to you|to this session|chris)\\b\",\n",
    "    ]\n",
    "    for pattern in filler_patterns:\n",
    "        text = re.sub(pattern, \"\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def chunk_transcript_by_sentences(text, max_sentences=4):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return [' '.join(sentences[i:i+max_sentences]) for i in range(0, len(sentences), max_sentences)]\n",
    "\n",
    "all_chunks = []\n",
    "all_video_ids = []\n",
    "all_subjects = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    transcript = row[\"text\"]\n",
    "    video_id = row[\"video_id\"]\n",
    "    subject = row[\"subject\"] \n",
    "    if pd.isna(transcript):\n",
    "        continue\n",
    "    cleaned = clean_text(transcript)\n",
    "    chunks = chunk_transcript_by_sentences(cleaned)\n",
    "    all_chunks.extend(chunks)\n",
    "    all_video_ids.extend([video_id] * len(chunks))\n",
    "    all_subjects.extend([subject] * len(chunks)) \n",
    "\n",
    "output_df = pd.DataFrame({\n",
    "    \"chunk_id\": [f\"chunk_{i}\" for i in range(len(all_chunks))],\n",
    "    \"video_id\": all_video_ids,\n",
    "    \"subject\": all_subjects,\n",
    "    \"text\": all_chunks\n",
    "})\n",
    "\n",
    "output_df.to_csv(\"../Data/processed_cleaned_chunks.csv\", index=False)\n",
    "output_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ae0d78",
   "metadata": {},
   "source": [
    "## Step3: Vector and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f29a298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "df = pd.read_csv(\"../data/processed_cleaned_chunks.csv\")\n",
    "\n",
    "langchain_docs = [\n",
    "    Document(\n",
    "        page_content=row[\"text\"],\n",
    "        metadata={\n",
    "            \"chunk_id\": row[\"chunk_id\"],\n",
    "            \"video_id\": row[\"video_id\"]\n",
    "        }\n",
    "    )\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "with open('../logs/chunk_previews.txt', 'w', encoding='utf-8') as f:\n",
    "    for i, doc in enumerate(langchain_docs):\n",
    "        preview = doc.page_content[:100].replace('\\n', ' ')\n",
    "        f.write(f\"[{i+1}] Chunk ID: {doc.metadata['chunk_id']} | Video ID: {doc.metadata['video_id']} | Text: {preview}...\\n\")\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "faiss_index = FAISS.from_documents(langchain_docs, embedding_model)\n",
    "\n",
    "faiss_index.save_local(str(FAISS_DIR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b853e732",
   "metadata": {},
   "source": [
    "## Step4: Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85542b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(df, df_processed):\n",
    "    \"\"\"Validate input and processed data.\"\"\"\n",
    "    duplicates = df[df.duplicated(subset=['video_id', 'youtube_link'], keep=False)]\n",
    "    if not duplicates.empty:\n",
    "        logging.warning(f'Found {len(duplicates)} duplicate video_id/youtube_link entries')\n",
    "    \n",
    "    invalid_links = df[~df['youtube_link'].str.contains(r'youtube\\.com|youtu\\.be', na=False)]\n",
    "    if not invalid_links.empty:\n",
    "        logging.warning(f'Found {len(invalid_links)} invalid YouTube links')\n",
    "    \n",
    "    chunk_lengths = df_processed['text'].str.len()\n",
    "    chunk_stats = {\n",
    "        'avg_length': chunk_lengths.mean(),\n",
    "        'min_length': chunk_lengths.min(),\n",
    "        'max_length': chunk_lengths.max()\n",
    "    }\n",
    "    logging.info(f'Chunk length stats: {chunk_stats}')\n",
    "    return chunk_stats\n",
    "\n",
    "chunk_stats = validate_data(df_clean, df_processed)\n",
    "with open(validation_report, 'a') as f:\n",
    "    f.write(f'Chunk Length Stats: Average={chunk_stats[\"avg_length\"]:.1f}, Min={chunk_stats[\"min_length\"]}, Max={chunk_stats[\"max_length\"]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcd6d7b",
   "metadata": {},
   "source": [
    "## Step5: Log Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d698da",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../logs/project_log.md', 'a') as f:\n",
    "    f.write('## Data Collection and Preprocessing\\n')\n",
    "    f.write('- Loaded YouTube video from `data/SNOW_YT_Videos.csv`.\\n')\n",
    "    f.write('- Loaded YouTube video metadata to `data/ServiceNow_Youtube_Metadata_Clean.csv`.\\n')\n",
    "    f.write('- Loaded YouTube video transcripts to `data/video_metadata_with_transcripts.csv`.\\n')\n",
    "    f.write('- Preprocessed transcripts with NLTK lemmatization and LangChain text splitting (chunk_size=500, overlap=50).\\n')\n",
    "    f.write(f'- Processed 22 videos, generating 328 chunks.\\n')\n",
    "    f.write('- Saved processed data to `data/processed_transcripts.csv`.\\n')\n",
    "    f.write('- Challenges: Resolved KeyError by standardizing column names (e.g., Number to video_id).\\n')\n",
    "    f.write('- Validation report saved to `logs/validation_report.txt`.\\n')\n",
    "    f.write('- Chunk Preview Data saved to `logs/chunk_preview.csv`.\\n')\n",
    "    f.write('- FAISS Store as faiss_store\\n')\n",
    "    f.write(f'  Average: {chunk_stats[\"avg_length\"]:.1f}\\n')\n",
    "    f.write(f'  Minimum: {chunk_stats[\"min_length\"]}\\n')\n",
    "    f.write(f'  Maximum: {chunk_stats[\"max_length\"]}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
