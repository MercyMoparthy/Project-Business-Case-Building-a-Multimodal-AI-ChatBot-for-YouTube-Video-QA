{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84c9e2a5",
   "metadata": {},
   "source": [
    "## Model: distilgpt2\n",
    "* Embedding - all-MiniLM-L6-v2 Model\n",
    "* Vectorizing - FAISS\n",
    "* LLM Pipeline - distilgpt2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f13b588",
   "metadata": {},
   "source": [
    "## Step1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3ab7901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import pipeline\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf474897",
   "metadata": {},
   "source": [
    "## Step2: Embedding & Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98469973",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "faiss_index = FAISS.load_local(\"../faiss_store\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "def retrieve_chunks(query, top_k=5):\n",
    "    docs = faiss_index.similarity_search(query, k=top_k)\n",
    "    return [doc.page_content for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08af7989",
   "metadata": {},
   "source": [
    "## Step3: Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c11a0843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is an AI agent in ServiceNow?\n",
      "Response: be helpful to you. We'll see if you can reach out to us and you can talk to us on Slack or Telegram or email us at hello@youraccount.io or we can send you anything you want to add in the upcoming development and we'll be on the lookout for more details about our upcoming development.\n"
     ]
    }
   ],
   "source": [
    "def generate_response(query, chunks):\n",
    "    \"\"\"Generate response using retrieved chunks and a lightweight LLM.\"\"\"\n",
    "    # Combine chunks into context\n",
    "    context = \"\\n\".join(chunks) if chunks else \"No relevant information found.\"\n",
    "    \n",
    "    # Define prompt template\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"context\", \"query\"],\n",
    "        template=\"Based on the following context, answer the query concisely:\\nContext: {context}\\nQuery: {query}\\nAnswer:\"\n",
    "    )\n",
    "    \n",
    "    # Format prompt\n",
    "    prompt = prompt_template.format(context=context, query=query)\n",
    "    \n",
    "    # Placeholder for lightweight LLM (e.g., Grok via xAI API)\n",
    "    # Replace with actual API call if available, or use HuggingFace model\n",
    "    try:\n",
    "        llm = pipeline('text-generation', model='distilgpt2', device=-1)\n",
    "        response = llm(prompt, max_length=150, truncation=True, do_sample=True, num_return_sequences=1)[0]['generated_text']\n",
    "        answer = response.split('Answer:')[-1].strip() if 'Answer:' in response else response.strip()\n",
    "    except Exception as e:\n",
    "        logging.error(f'LLM request failed: {e}')\n",
    "        answer = \"Error generating response. Using context directly:\\n\" + context[:200]\n",
    "    \n",
    "    logging.info(f'Generated response for query: {query}')\n",
    "    return answer\n",
    "\n",
    "# Example usage\n",
    "query = \"What is an AI agent in ServiceNow?\"\n",
    "chunks = retrieve_chunks(query)\n",
    "response = generate_response(query, chunks)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6ebb22",
   "metadata": {},
   "source": [
    "## Step 4: Test Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98557afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is ITSM in ServiceNow?\n",
      "A: or the platform ui.\n",
      "Conclusion\n",
      "As you can see, all this is the one thing that needs to be done. As long as you are using your own service you can do this and get the job done.\n",
      "In the event of any problem and any future problems you may have, please take this step and let us know in the comments!\n",
      "If you would like to hear more, or if you would like to learn more, read the article here.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_queries = [\n",
    "    \"What is ITSM in ServiceNow?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    chunks = retrieve_chunks(query)\n",
    "    result = generate_response(query, chunks)\n",
    "    print(f\"Q: {query}\\nA: {result}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8af15871",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Explain CMDB relationships.\n",
      "A: it.\n",
      "Now, let's have a look at the command prompt commands for the following command:\n",
      "cmds.yml \"C:\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86)\\Program Files (x86\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_queries = [\n",
    "    \"Explain CMDB relationships.\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    chunks = retrieve_chunks(query)\n",
    "    result = generate_response(query, chunks)\n",
    "    print(f\"Q: {query}\\nA: {result}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1069745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: How does Incident Management work?\n",
      "A: .\n",
      "But a more concrete and concrete example that this is not being addressed is the recent release of the Microsoft-funded product for mobile, which is very much a product of Microsoft, that was a big step towards Microsoft's leadership. A new service called Microsoft App Service was released today, and we will be on the topic for a longer time.\n",
      "Note that Microsoft App Service is not only a product of Microsoft, it is a product of Microsoft, it is a product of Microsoft, it is a product of Microsoft, it is a product of Microsoft, it is a product of Microsoft, it is a product of Microsoft, it is a product of Microsoft, it is a product of Microsoft, it is a product of Microsoft, it is a product of Microsoft, it is a product of Microsoft, it is a product of Microsoft, it is a product of Microsoft, it is a product of Microsoft, it is a product of Microsoft, it is a product of Microsoft, it is a product of Microsoft, it is a product of Microsoft, it is a product of Microsoft, it is a product of Microsoft, it is a product of Microsoft, it is a product of Microsoft, it is a product of Microsoft, it is a product of Microsoft, it is a product\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_queries = [\n",
    "    \"How does Incident Management work?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    chunks = retrieve_chunks(query)\n",
    "    result = generate_response(query, chunks)\n",
    "    print(f\"Q: {query}\\nA: {result}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
