{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84c9e2a5",
   "metadata": {},
   "source": [
    "## Model: distilgpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f67536a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: pinecone-client 3.0.0Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Uninstalling pinecone-client-3.0.0:\n",
      "  Successfully uninstalled pinecone-client-3.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y pinecone-client\n",
    "%pip install pinecone-client==3.0.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3ab7901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pinecone\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import torch\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2607d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_INDEX_NAME = \"youtube-transcripts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5ad5322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc8ff207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(PINECONE_INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e4e6791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_chunks(query, top_k=5):\n",
    "    query_embedding = embedder.encode(query, show_progress_bar=False).tolist()\n",
    "    results = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n",
    "    chunks = []\n",
    "    for match in results[\"matches\"]:\n",
    "        text = match[\"metadata\"].get(\"text\")\n",
    "        if text:\n",
    "            chunks.append(text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "272786fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, chunks):\n",
    "    context = \"\\n\".join(chunks)\n",
    "    prompt = f\"Answer the question based on the context:\\nContext: {context}\\nQuestion: {query}\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=150)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12b464ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = [\n",
    "    \"What is ITSM in ServiceNow?\",\n",
    "    \"Explain CMDB relationships.\",\n",
    "    \"How does Incident Management work?\"\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "366e1d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (595 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is ITSM in ServiceNow?\n",
      "A: event management integration\n",
      "\n",
      "Q: Explain CMDB relationships.\n",
      "A: a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a.\n",
      "\n",
      "Q: How does Incident Management work?\n",
      "A: proactive versus reactive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in test_queries:\n",
    "    try:\n",
    "        chunks = retrieve_chunks(query)\n",
    "        response = generate_response(query, chunks)\n",
    "        print(f\"Q: {query}\\nA: {response}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error for query '{query}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7995beba",
   "metadata": {},
   "source": [
    "## Results Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6507c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will save results to: c:\\Users\\Mercy\\OneDrive\\Documents\\ServiceNow classes\\OneDrive\\Project-Business-Case-Building-a-Multimodal-AI-ChatBot-for-YouTube-Video-QA\\results\\model_outputs.csv\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ROOT = r\"c:\\Users\\Mercy\\OneDrive\\Documents\\ServiceNow classes\\OneDrive\\Project-Business-Case-Building-a-Multimodal-AI-ChatBot-for-YouTube-Video-QA\"\n",
    "RESULTS_DIR = os.path.join(PROJECT_ROOT, 'results')\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "OUTPUT_FILE = os.path.join(RESULTS_DIR, 'model_outputs.csv')\n",
    "print(\"Will save results to:\", OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cffa4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\Mercy\\OneDrive\\Documents\\ServiceNow classes\\OneDrive\\Project-Business-Case-Building-a-Multimodal-AI-ChatBot-for-YouTube-Video-QA\\Sample Models\n",
      "Writing results to: c:\\Users\\Mercy\\OneDrive\\Documents\\ServiceNow classes\\OneDrive\\Project-Business-Case-Building-a-Multimodal-AI-ChatBot-for-YouTube-Video-QA\\results\\model_outputs.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Writing results to:\", OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428ccade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "def log_model_output(\n",
    "    notebook,\n",
    "    query,\n",
    "    model_answer,\n",
    "    output_file= OUTPUT_FILE,\n",
    "    additional_fields=None\n",
    "):\n",
    "    # Compose data row\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    row = {\n",
    "        'timestamp': timestamp,\n",
    "        'notebook': notebook,\n",
    "        'query': query,\n",
    "        'model_answer': model_answer\n",
    "    }\n",
    "    # Add additional fields if provided\n",
    "    if additional_fields:\n",
    "        row.update(additional_fields)\n",
    "    # Write header if file does not exist\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    file_exists = os.path.isfile(output_file)\n",
    "    with open(output_file, 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=row.keys())\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0183fb",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7c4174e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is ITSM in ServiceNow?\",\n",
    "    \"Explain CMDB relationships.\",\n",
    "    \"How does Incident Management work?\"\n",
    "]\n",
    "\n",
    "reference_answers = [\n",
    "    \"ITSM (IT Service Management) in ServiceNow refers to the strategic approach for designing, delivering, managing, and improving the way IT is used within an organization. ServiceNow ITSM offers a cloud-based platform to automate and streamline core IT processes such as Incident Management, Problem Management, Change Management, Request Fulfillment, and Knowledge Management. It enhances service quality, reduces operational costs, and improves user satisfaction through intelligent workflows, analytics, and a centralized service portal. ITSM in ServiceNow aligns IT services with business goals, ensuring efficient and consistent service delivery.\",\n",
    "    \"CMDB (Configuration Management Database) relationships in ServiceNow define how Configuration Items (CIs) are connected and interact with each other. These relationships help build a dependency map that visually represents infrastructure, services, and their interconnections. Common relationship types include 'Depends on / Used by', 'Runs on / Hosted on', and 'Connected to'. For example, an application may run on a server, which in turn depends on a specific database. These relationships are essential for impact analysis, change planning, and root cause analysis, enabling IT teams to understand how changes or incidents affect related systems.\",\n",
    "    \"Incident Management in ServiceNow focuses on restoring normal service operations as quickly as possible after an unplanned interruption. When an incident is reported—via the service portal, email, or phone—it is automatically logged and categorized. The system assigns priority based on impact and urgency, then routes the ticket to the appropriate support group. Technicians investigate, resolve the issue, and document the resolution. Users are kept informed through notifications. Once resolved, the incident is closed after verification. Incident Management helps minimize downtime, improves user satisfaction, and provides valuable data for trend analysis and continual service improvement.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e1294a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_answers = [\n",
    "    \"event management integration\",\n",
    "    \"a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a. a.\",\n",
    "    \"proactive versus reactive\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9688b154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match(prediction, reference):\n",
    "    \"\"\"Returns 1 if the prediction matches the reference exactly (case-insensitive, stripped), else 0.\"\"\"\n",
    "    return int(prediction.strip().lower() == reference.strip().lower())\n",
    "\n",
    "def f1_score(prediction, reference):\n",
    "    \"\"\"Compute token-level F1 score between prediction and reference.\"\"\"\n",
    "    pred_tokens = prediction.lower().split()\n",
    "    ref_tokens = reference.lower().split()\n",
    "    common = set(pred_tokens) & set(ref_tokens)\n",
    "    if not common:\n",
    "        return 0.0\n",
    "    precision = len(common) / len(pred_tokens)\n",
    "    recall = len(common) / len(ref_tokens)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def rouge_l_score(prediction, reference):\n",
    "    \"\"\"Compute a simple ROUGE-L score based on longest common subsequence.\"\"\"\n",
    "    def lcs(X, Y):\n",
    "        m = len(X)\n",
    "        n = len(Y)\n",
    "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                if X[i] == Y[j]:\n",
    "                    dp[i + 1][j + 1] = dp[i][j] + 1\n",
    "                else:\n",
    "                    dp[i + 1][j + 1] = max(dp[i][j + 1], dp[i + 1][j])\n",
    "        return dp[m][n]\n",
    "    pred_tokens = prediction.lower().split()\n",
    "    ref_tokens = reference.lower().split()\n",
    "    lcs_len = lcs(pred_tokens, ref_tokens)\n",
    "    if len(ref_tokens) == 0 or len(pred_tokens) == 0:\n",
    "        return 0.0\n",
    "    recall = lcs_len / len(ref_tokens)\n",
    "    precision = lcs_len / len(pred_tokens)\n",
    "    if recall + precision == 0:\n",
    "        return 0.0\n",
    "    return 2 * recall * precision / (recall + precision)\n",
    "\n",
    "def bleu_score(prediction, reference):\n",
    "    \"\"\"Compute a simple BLEU-1 score (unigram precision).\"\"\"\n",
    "    pred_tokens = prediction.lower().split()\n",
    "    ref_tokens = reference.lower().split()\n",
    "    if not pred_tokens or not ref_tokens:\n",
    "        return 0.0\n",
    "    matches = sum(1 for token in pred_tokens if token in ref_tokens)\n",
    "    return matches / len(pred_tokens)\n",
    "\n",
    "audio_filename = None  # or set to a placeholder string if needed\n",
    "\n",
    "for question, ref_answer in zip(questions, reference_answers):\n",
    "    # Retrieve relevant chunks for the question\n",
    "    chunks = retrieve_chunks(question)\n",
    "    answer = generate_response(question, chunks)\n",
    "    em = exact_match(answer, ref_answer)\n",
    "    f1 = f1_score(answer, ref_answer)\n",
    "    rouge = rouge_l_score(answer, ref_answer)\n",
    "    bleu = bleu_score(answer, ref_answer)\n",
    "    # Remove confidence since generate_response does not return it\n",
    "    log_model_output(\n",
    "        notebook='3c_model_test_flat-t5-base',\n",
    "        query=question,\n",
    "        model_answer=answer,\n",
    "        additional_fields={'em': em, 'f1': f1, 'rougeL': rouge, 'bleu': bleu}\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
