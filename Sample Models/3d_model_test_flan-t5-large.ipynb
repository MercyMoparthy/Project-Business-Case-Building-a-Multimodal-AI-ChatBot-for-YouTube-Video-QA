{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84c9e2a5",
   "metadata": {},
   "source": [
    "## Model: google/flan-t5-large\n",
    "* Vector: Pinecone\n",
    "* Enbedding:all-MiniLM-L6-v2\n",
    "* Content Retrieval\n",
    "* Results Evaluation\n",
    "* Metrics (Exact Match, f1, Rouge , bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e2bdd9",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f67536a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pinecone-client==3.0.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3ab7901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_text_splitters.character import RecursiveCharacterTextSplitter\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2607d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_INDEX_NAME = \"youtube-transcripts\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4103e42",
   "metadata": {},
   "source": [
    "## Embedding & Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5ad5322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc8ff207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(PINECONE_INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910d49e1",
   "metadata": {},
   "source": [
    "## Content Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e4e6791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_chunks(query, top_k=5):\n",
    "    query_embedding = embedder.encode(query, show_progress_bar=False).tolist()\n",
    "    results = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n",
    "    chunks = []\n",
    "    for match in results[\"matches\"]:\n",
    "        text = match[\"metadata\"].get(\"text\")\n",
    "        if text:\n",
    "            chunks.append(text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c047793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, chunks):\n",
    "    context = \"\\n\".join(chunks)\n",
    "    prompt = f\"Answer the question based on the context:\\nContext: {context}\\nQuestion: {query}\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=150)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f04fec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = [\n",
    "    \"What is ITSM in ServiceNow?\",\n",
    "    \"Explain CMDB relationships.\",\n",
    "    \"How does Incident Management work?\",\n",
    "    \"what are the AI capabilities in ServiceNow?\"  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce44821d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (595 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is ITSM in ServiceNow?\n",
      "A: event management\n",
      "\n",
      "Q: Explain CMDB relationships.\n",
      "A: The information that we have within the CMDB and sort of like the workflow and post mortems and like previous incident and alert and we can marry that together and give the operator sort of like the best suggestion on what the next step should be\n",
      "\n",
      "Q: How does Incident Management work?\n",
      "A: proactive versus reactive\n",
      "\n",
      "Q: what are the AI capabilities in ServiceNow?\n",
      "A: visual ai\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in test_queries:\n",
    "    try:\n",
    "        chunks = retrieve_chunks(query)\n",
    "        response = generate_response(query, chunks)\n",
    "        print(f\"Q: {query}\\nA: {response}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error for query '{query}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761c6272",
   "metadata": {},
   "source": [
    "## Results Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "426bf945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will save results to: c:\\Users\\Mercy\\OneDrive\\Documents\\ServiceNow classes\\OneDrive\\Project-Business-Case-Building-a-Multimodal-AI-ChatBot-for-YouTube-Video-QA\\results\\model_outputs.csv\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ROOT = r\"c:\\Users\\Mercy\\OneDrive\\Documents\\ServiceNow classes\\OneDrive\\Project-Business-Case-Building-a-Multimodal-AI-ChatBot-for-YouTube-Video-QA\"\n",
    "RESULTS_DIR = os.path.join(PROJECT_ROOT, 'results')\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "OUTPUT_FILE = os.path.join(RESULTS_DIR, 'model_outputs.csv')\n",
    "print(\"Will save results to:\", OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad552d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\Mercy\\OneDrive\\Documents\\ServiceNow classes\\OneDrive\\Project-Business-Case-Building-a-Multimodal-AI-ChatBot-for-YouTube-Video-QA\\Sample Models\n",
      "Writing results to: c:\\Users\\Mercy\\OneDrive\\Documents\\ServiceNow classes\\OneDrive\\Project-Business-Case-Building-a-Multimodal-AI-ChatBot-for-YouTube-Video-QA\\results\\model_outputs.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Writing results to:\", OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb33fef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "def log_model_output(\n",
    "    notebook,\n",
    "    query,\n",
    "    model_answer,\n",
    "    output_file= OUTPUT_FILE,\n",
    "    additional_fields=None\n",
    "):\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    row = {\n",
    "        'timestamp': timestamp,\n",
    "        'notebook': notebook,\n",
    "        'query': query,\n",
    "        'model_answer': model_answer\n",
    "    }\n",
    "    if additional_fields:\n",
    "        row.update(additional_fields)\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    file_exists = os.path.isfile(output_file)\n",
    "    with open(output_file, 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=row.keys())\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddba05c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is ITSM in ServiceNow?\",\n",
    "    \"Explain CMDB relationships.\",\n",
    "    \"How does Incident Management work?\"\n",
    "]\n",
    "\n",
    "reference_answers = [\n",
    "    \"ITSM (IT Service Management) in ServiceNow refers to the strategic approach for designing, delivering, managing, and improving the way IT is used within an organization. ServiceNow ITSM offers a cloud-based platform to automate and streamline core IT processes such as Incident Management, Problem Management, Change Management, Request Fulfillment, and Knowledge Management. It enhances service quality, reduces operational costs, and improves user satisfaction through intelligent workflows, analytics, and a centralized service portal. ITSM in ServiceNow aligns IT services with business goals, ensuring efficient and consistent service delivery.\",\n",
    "    \"CMDB (Configuration Management Database) relationships in ServiceNow define how Configuration Items (CIs) are connected and interact with each other. These relationships help build a dependency map that visually represents infrastructure, services, and their interconnections. Common relationship types include 'Depends on / Used by', 'Runs on / Hosted on', and 'Connected to'. For example, an application may run on a server, which in turn depends on a specific database. These relationships are essential for impact analysis, change planning, and root cause analysis, enabling IT teams to understand how changes or incidents affect related systems.\",\n",
    "    \"Incident Management in ServiceNow focuses on restoring normal service operations as quickly as possible after an unplanned interruption. When an incident is reported—via the service portal, email, or phone—it is automatically logged and categorized. The system assigns priority based on impact and urgency, then routes the ticket to the appropriate support group. Technicians investigate, resolve the issue, and document the resolution. Users are kept informed through notifications. Once resolved, the incident is closed after verification. Incident Management helps minimize downtime, improves user satisfaction, and provides valuable data for trend analysis and continual service improvement.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8843b7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_answers = [\n",
    "    \"event management\",\n",
    "    \"The information that we have within the CMDB and sort of like the workflow and post mortems and like previous incident and alert and we can marry that together and give the operator sort of like the best suggestion on what the next step should be\",\n",
    "    \"proactive versus reactive\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dd4315",
   "metadata": {},
   "source": [
    "## Metrics (Exact Match, f1, Rouge , bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194b5b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match(prediction, reference):\n",
    "    \"\"\"Returns 1 if the prediction matches the reference exactly (case-insensitive, stripped), else 0.\"\"\"\n",
    "    return int(prediction.strip().lower() == reference.strip().lower())\n",
    "\n",
    "def f1_score(prediction, reference):\n",
    "    \"\"\"Compute token-level F1 score between prediction and reference.\"\"\"\n",
    "    pred_tokens = prediction.lower().split()\n",
    "    ref_tokens = reference.lower().split()\n",
    "    common = set(pred_tokens) & set(ref_tokens)\n",
    "    if not common:\n",
    "        return 0.0\n",
    "    precision = len(common) / len(pred_tokens)\n",
    "    recall = len(common) / len(ref_tokens)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def rouge_l_score(prediction, reference):\n",
    "    \"\"\"Compute a simple ROUGE-L score based on longest common subsequence.\"\"\"\n",
    "    def lcs(X, Y):\n",
    "        m = len(X)\n",
    "        n = len(Y)\n",
    "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                if X[i] == Y[j]:\n",
    "                    dp[i + 1][j + 1] = dp[i][j] + 1\n",
    "                else:\n",
    "                    dp[i + 1][j + 1] = max(dp[i][j + 1], dp[i + 1][j])\n",
    "        return dp[m][n]\n",
    "    pred_tokens = prediction.lower().split()\n",
    "    ref_tokens = reference.lower().split()\n",
    "    lcs_len = lcs(pred_tokens, ref_tokens)\n",
    "    if len(ref_tokens) == 0 or len(pred_tokens) == 0:\n",
    "        return 0.0\n",
    "    recall = lcs_len / len(ref_tokens)\n",
    "    precision = lcs_len / len(pred_tokens)\n",
    "    if recall + precision == 0:\n",
    "        return 0.0\n",
    "    return 2 * recall * precision / (recall + precision)\n",
    "\n",
    "def bleu_score(prediction, reference):\n",
    "    \"\"\"Compute a simple BLEU-1 score (unigram precision).\"\"\"\n",
    "    pred_tokens = prediction.lower().split()\n",
    "    ref_tokens = reference.lower().split()\n",
    "    if not pred_tokens or not ref_tokens:\n",
    "        return 0.0\n",
    "    matches = sum(1 for token in pred_tokens if token in ref_tokens)\n",
    "    return matches / len(pred_tokens)\n",
    "\n",
    "audio_filename = None\n",
    "for question, ref_answer in zip(questions, reference_answers):\n",
    "    # Retrieve relevant chunks for the question\n",
    "    chunks = retrieve_chunks(question)\n",
    "    answer = generate_response(question, chunks)\n",
    "    em = exact_match(answer, ref_answer)\n",
    "    f1 = f1_score(answer, ref_answer)\n",
    "    rouge = rouge_l_score(answer, ref_answer)\n",
    "    bleu = bleu_score(answer, ref_answer)\n",
    "    log_model_output(\n",
    "        notebook='3c_model_test_flat-t5-large',\n",
    "        query=question,\n",
    "        model_answer=answer,\n",
    "        additional_fields={'em': em, 'f1': f1, 'rougeL': rouge, 'bleu': bleu}\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
